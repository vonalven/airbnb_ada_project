{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from IPython.core.display import display\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PUBLIC TRANSPORTATION DATA SET DOWNLOAD\n",
    "By launching this script, the https://www.citylines.co/data website is scraped and the last updated version of the transports json data sets are downloaded. A still-in-work part will allow to merge all the downloaded datasets to a more useful unique .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Looking for [chromedriver 78.0.3904.105 mac64] driver in cache \n",
      "File found in cache by path [/Users/sirinesayagh/.wdm/drivers/chromedriver/78.0.3904.105/mac64/chromedriver]\n"
     ]
    }
   ],
   "source": [
    "##########################################\n",
    "### MODIFY THIS, to store the DataSet: ###\n",
    "##########################################\n",
    "\n",
    "localDataUrl = '/Volumes/Disk2/Courses MA3/MA3 - ADA/AIRBNB data/Transports'\n",
    "\n",
    "##########################################\n",
    "##########################################\n",
    "\n",
    "url = 'https://www.citylines.co/data'\n",
    "\n",
    "# get all the files names and the files urls\n",
    "files_df = scrapeCityLines(url)\n",
    "# download the data set\n",
    "download_data(files_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapeCityLines(url):\n",
    "    # Selenium is able to simulate the browser, and so we can make it wait until \n",
    "    # the page finished loading before we are getting the data.\n",
    "    driver   = webdriver.Chrome(ChromeDriverManager().install())\n",
    "    response = driver.get(url)\n",
    "    select   = Select(driver.find_element_by_class_name('c-field'))\n",
    "\n",
    "    links_download = []\n",
    "    files_names    = []\n",
    "    idx            = 0\n",
    "\n",
    "    for cities in select.options:\n",
    "\n",
    "        # first element of the dropdown menu is Select City; this is not a city!!!\n",
    "\n",
    "        if idx >= 1:\n",
    "            opt   = cities.click()\n",
    "            soup  = BeautifulSoup(driver.page_source)\n",
    "            links = soup.find_all('a', class_ = 'c-link')\n",
    "\n",
    "            for i in links:\n",
    "                if i.has_attr('href') and ('/api/' in i['href']):\n",
    "                    links_download.append('https://www.citylines.co' + i['href'])\n",
    "                    files_names.append(i['download'])\n",
    "        idx += 1\n",
    "    \n",
    "    driver.quit()\n",
    "\n",
    "    files_df = pd.DataFrame({'file_name':files_names, 'link':links_download})\n",
    "    \n",
    "    return files_df\n",
    "\n",
    "def download_data(files_df):\n",
    "    # download all the dataset\n",
    "    for index, row in files_df.iterrows():\n",
    "\n",
    "            file_name = localDataUrl + '/' + row['file_name']\n",
    "            print('Downloading... ' + str(index+1) +'/' + str(files_df.shape[0]) + ' : ' + file_name)\n",
    "\n",
    "            urllib.request.urlretrieve(row['link'], file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORK in progress....\n",
    "\"\"\"\"\n",
    "from pandas.io.json import json_normalize \n",
    "import json\n",
    "\n",
    "def mergeFiles(file_name, files_folder_path, main_node):\n",
    "    \n",
    "    \n",
    "    all_files = os.listdir(files_folder_path)\n",
    "    selected_files = [i for i in all_files if file_name in i]\n",
    "    \n",
    "    df_merged = pd.DataFrame()\n",
    "    \n",
    "    for fl in selected_files:\n",
    "        print('Merging file... ' + fl)\n",
    "        with open(files_folder_path + '/' + fl) as data_file:    \n",
    "            data = json.load(data_file)\n",
    "        df = pd.DataFrame()\n",
    "    \n",
    "        for i in data[main_node]:\n",
    "            df = pd.concat([df, json_normalize(i)], sort = True)\n",
    "\n",
    "        df = df.reset_index(drop = True)\n",
    "\n",
    "        # further expand all left dict in columns\n",
    "        cols_with_dict = []\n",
    "        for j in df.columns:\n",
    "            hasDict = \"<class 'dict'>\" in df[j].apply(lambda x: str(type(x[0])) if hasattr(x, '__len__') and len(x)> 0 else x).tolist()\n",
    "            cols_with_dict.append(hasDict)\n",
    "\n",
    "        for a, b in zip(cols_with_dict, df.columns):\n",
    "            if a:\n",
    "                dicts = [x[0] if len(x)>0 else {} for x in df[b]]\n",
    "                df = pd.concat([df, json_normalize(records)], axis = 1, sort = True).drop(b, axis = 1)\n",
    "\n",
    "        df['city'] = fl.split('_')[0]\n",
    "        df.columns = [s.split('.')[-1] for s in df.columns.values]\n",
    "        \n",
    "        \n",
    "        dfs = [df, df_merged]\n",
    "        if df_merged.shape[0] > 0:\n",
    "            df_merged = pd.concat(dfs, ignore_index=True)\n",
    "        else:\n",
    "            df_merged = df\n",
    "        #df_merged = pd.concat([df, df_merged], sort = True, ignore_index = True)\n",
    "        \n",
    "    \n",
    "    return df_merged\n",
    "\n",
    "\n",
    "\n",
    "df_stations = mergeFiles('stations.geojson', files_folder_path = localDataUrl, main_node = 'features')\n",
    "df_stations\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nested json files handling\n",
    "Here, the Amsterdam nested json stations is converted to a pandas DataFrame. The goal will be to automatize this and do it for all the downloaded data sets. The above code is intended to do it but is still to debug. At the end the goal is to have an unique .csv file comprising all the stations for all the cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TESTS\n",
    "import json\n",
    "from pandas.io.json import json_normalize \n",
    "\n",
    "with open('./Transports/amsterdam_stations.geojson') as data_file:    \n",
    "            data = json.load(data_file)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for i in data['features']:\n",
    "    df = pd.concat([df, json_normalize(i)], sort = True)\n",
    "    \n",
    "df = df.reset_index(drop = True)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "cols_with_dict = []\n",
    "for i in df.columns:\n",
    "    hasDict = \"<class 'dict'>\" in df[i].apply(lambda x: str(type(x[0])) if hasattr(x, '__len__') and len(x)> 0 else x).tolist()\n",
    "    cols_with_dict.append(hasDict)\n",
    "\n",
    "for a, b in zip(cols_with_dict, df.columns):\n",
    "    if a:\n",
    "        dicts = [x[0] if len(x)>0 else {} for x in df[b]]\n",
    "        df = pd.concat([df, json_normalize(dicts)], axis = 1, sort = True).drop(b, axis = 1)\n",
    "\n",
    "df.columns = [s.split('.')[-1] for s in df.columns.values]  \n",
    "\n",
    "df.to_csv('Amsterdam_stations.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIND CITIES COMMUN TO BOTH DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['grenoble',\n",
       " 'mulhouse',\n",
       " 'savannah',\n",
       " 'madrid',\n",
       " 'los angeles',\n",
       " 'besancon',\n",
       " 'chicago',\n",
       " 'innsbruck',\n",
       " 'orleans',\n",
       " 'le mans',\n",
       " 'tours',\n",
       " 'new york',\n",
       " 'valencia',\n",
       " 'dzaoudzi',\n",
       " 'bilbao',\n",
       " 'reims',\n",
       " 'dallas',\n",
       " 'guadalajara',\n",
       " 'rennes',\n",
       " 'edinburgh',\n",
       " 'cincinnati',\n",
       " 'osaka',\n",
       " 'rosario',\n",
       " 'ottawa',\n",
       " 'lille',\n",
       " 'beijing',\n",
       " 'kuala lumpur',\n",
       " 'valparaiso',\n",
       " 'buenos aires',\n",
       " 'dijon',\n",
       " 'toulouse',\n",
       " 'jakarta',\n",
       " 'atlanta',\n",
       " 'florianopolis',\n",
       " 'sao paulo',\n",
       " 'munich',\n",
       " 'richmond',\n",
       " 'boston',\n",
       " 'rouen',\n",
       " 'strasbourg',\n",
       " 'amsterdam',\n",
       " 'rome',\n",
       " 'barcelona',\n",
       " 'montreal',\n",
       " 'lisbon',\n",
       " 'genoa',\n",
       " 'bordeaux',\n",
       " 'newcastle on tyne',\n",
       " 'marseilles',\n",
       " 'shanghai',\n",
       " 'graz',\n",
       " 'tokyo',\n",
       " 'prague',\n",
       " 'nice',\n",
       " 'la paz',\n",
       " 'naples',\n",
       " 'washington',\n",
       " 'caracas',\n",
       " 'san francisco',\n",
       " 'bogota',\n",
       " 'lima',\n",
       " 'milan',\n",
       " 'lyons',\n",
       " 'hong kong',\n",
       " 'mumbai',\n",
       " 'paris',\n",
       " 'mexico city',\n",
       " 'le havre',\n",
       " 'montpellier',\n",
       " 'rio de janeiro',\n",
       " 'budapest',\n",
       " 'singapore',\n",
       " 'sydney',\n",
       " 'pune',\n",
       " 'brussels',\n",
       " 'portland united states',\n",
       " 'brest',\n",
       " 'berlin',\n",
       " 'venice',\n",
       " 'manchester',\n",
       " 'quito',\n",
       " 'vienna',\n",
       " 'toronto',\n",
       " 'concepcion',\n",
       " 'nantes',\n",
       " 'norfolk',\n",
       " 'istanbul',\n",
       " 'melbourne',\n",
       " 'san sebastian',\n",
       " 'warsaw',\n",
       " 'salvador',\n",
       " 'clermont ferrand',\n",
       " 'nancy',\n",
       " 'winnipeg',\n",
       " 'santiago de chile',\n",
       " 'stockholm',\n",
       " 'manila',\n",
       " 'angers',\n",
       " 'glasgow',\n",
       " 'charlotte',\n",
       " 'london',\n",
       " 'seattle',\n",
       " 'zaragoza']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get cities from transport data set\n",
    "all_transport_files = os.listdir(\"./Transports\") \n",
    "transport_cities = [file.split('_')[0] for file in all_files]\n",
    "transport_cities = list(set(transport_cities)) #drop duplicate because more than one file per city\n",
    "transport_cities = [x.replace('-',' ') for x in transport_cities] #remove hyphen\n",
    "transport_cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['store',\n",
       " 'washington, d.c.',\n",
       " 'mallorca',\n",
       " 'belize',\n",
       " 'victoria',\n",
       " 'mexico city',\n",
       " 'edinburgh',\n",
       " 'melbourne',\n",
       " 'manchester',\n",
       " 'jersey city',\n",
       " 'beijing',\n",
       " 'london',\n",
       " 'san mateo county',\n",
       " 'rio de janeiro',\n",
       " 'broward county',\n",
       " 'oslo',\n",
       " 'venice',\n",
       " 'girona',\n",
       " 'vancouver',\n",
       " 'porto',\n",
       " 'quebec city',\n",
       " 'trentino',\n",
       " 'copenhagen',\n",
       " 'oakland',\n",
       " 'portland',\n",
       " 'naples',\n",
       " 'sevilla',\n",
       " 'ottawa',\n",
       " 'lyon',\n",
       " 'los angeles',\n",
       " 'istanbul',\n",
       " 'munich',\n",
       " 'paris',\n",
       " 'toronto',\n",
       " 'san diego',\n",
       " 'malaga',\n",
       " 'columbus',\n",
       " 'barossa valley',\n",
       " 'san francisco',\n",
       " 'brussels',\n",
       " 'barcelona',\n",
       " 'thessaloniki',\n",
       " 'bordeaux',\n",
       " 'ghent',\n",
       " 'tasmania',\n",
       " 'boston',\n",
       " 'barwon south west, vic',\n",
       " 'cape town',\n",
       " 'new york city',\n",
       " 'florence',\n",
       " 'cambridge',\n",
       " 'geneva',\n",
       " 'dublin',\n",
       " 'western australia',\n",
       " 'taipei',\n",
       " 'milan',\n",
       " 'lisbon',\n",
       " 'salem, or',\n",
       " 'ireland',\n",
       " 'menorca',\n",
       " 'stockholm',\n",
       " 'berlin',\n",
       " 'nashville',\n",
       " 'denver',\n",
       " 'valencia',\n",
       " 'crete',\n",
       " 'greater manchester',\n",
       " 'madrid',\n",
       " 'sicily',\n",
       " 'sydney',\n",
       " 'asheville',\n",
       " 'bristol',\n",
       " 'austin',\n",
       " 'south aegean',\n",
       " 'tokyo',\n",
       " 'bergamo',\n",
       " 'hong kong',\n",
       " 'northern rivers',\n",
       " 'vienna',\n",
       " 'seattle',\n",
       " 'antwerp',\n",
       " 'clark county, nv',\n",
       " 'amsterdam',\n",
       " 'euskadi',\n",
       " 'twin cities msa',\n",
       " 'buenos aires',\n",
       " 'rhode island',\n",
       " 'puglia',\n",
       " 'santa cruz county',\n",
       " 'athens',\n",
       " 'pacific grove',\n",
       " 'prague',\n",
       " 'vaud',\n",
       " 'new orleans',\n",
       " 'montreal',\n",
       " 'singapore',\n",
       " 'rome',\n",
       " 'bologna',\n",
       " 'new brunswick',\n",
       " 'chicago',\n",
       " 'santiago',\n",
       " 'hawaii',\n",
       " 'santa clara county']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get cities from airbnb data set\n",
    "all_airbnb_files = os.listdir(\"./data\") \n",
    "airbnb_cities = [file.split('_')[1] for file in all_airbnb_files]\n",
    "airbnb_cities = list(set(airbnb_cities)) #drop duplicate because more than one file per city\n",
    "airbnb_cities = [x.lower() for x in airbnb_cities] #remove capital letters\n",
    "airbnb_cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "commun_cities = list(set(transport_cities).intersection(set(airbnb_cities)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['manchester',\n",
       " 'beijing',\n",
       " 'madrid',\n",
       " 'vienna',\n",
       " 'toronto',\n",
       " 'tokyo',\n",
       " 'prague',\n",
       " 'naples',\n",
       " 'chicago',\n",
       " 'istanbul',\n",
       " 'munich',\n",
       " 'melbourne',\n",
       " 'milan',\n",
       " 'boston',\n",
       " 'paris',\n",
       " 'amsterdam',\n",
       " 'san francisco',\n",
       " 'buenos aires',\n",
       " 'singapore',\n",
       " 'stockholm',\n",
       " 'valencia',\n",
       " 'rome',\n",
       " 'barcelona',\n",
       " 'hong kong',\n",
       " 'montreal',\n",
       " 'los angeles',\n",
       " 'london',\n",
       " 'seattle',\n",
       " 'rio de janeiro',\n",
       " 'sydney',\n",
       " 'lisbon',\n",
       " 'edinburgh',\n",
       " 'mexico city',\n",
       " 'brussels',\n",
       " 'berlin',\n",
       " 'venice',\n",
       " 'ottawa',\n",
       " 'bordeaux']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "commun_cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
