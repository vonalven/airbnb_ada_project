{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from IPython.core.display import display\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PUBLIC TRANSPORTATION DATA SET DOWNLOAD\n",
    "By launching this script, the https://www.citylines.co/data website is scraped and the last updated version of the transports json data sets are downloaded. A still-in-work part will allow to merge all the downloaded datasets to a more useful unique .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Looking for [chromedriver 78.0.3904.105 mac64] driver in cache \n",
      "File found in cache by path [/Users/Giacomo/.wdm/drivers/chromedriver/78.0.3904.105/mac64/chromedriver]\n"
     ]
    }
   ],
   "source": [
    "##########################################\n",
    "### MODIFY THIS, to store the DataSet: ###\n",
    "##########################################\n",
    "\n",
    "localDataUrl = '/Volumes/Disk2/Courses MA3/MA3 - ADA/AIRBNB data/Transports'\n",
    "\n",
    "##########################################\n",
    "##########################################\n",
    "\n",
    "url = 'https://www.citylines.co/data'\n",
    "\n",
    "# get all the files names and the files urls\n",
    "files_df = scrapeCityLines(urls)\n",
    "# download the data set\n",
    "download_data(files_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapeCityLines(url):\n",
    "    # Selenium is able to simulate the browser, and so we can make it wait until \n",
    "    # the page finished loading before we are getting the data.\n",
    "    driver   = webdriver.Chrome(ChromeDriverManager().install())\n",
    "    response = driver.get(url)\n",
    "    select   = Select(driver.find_element_by_class_name('c-field'))\n",
    "\n",
    "    links_download = []\n",
    "    files_names    = []\n",
    "    idx            = 0\n",
    "\n",
    "    for cities in select.options:\n",
    "\n",
    "        # first element of the dropdown menu is Select City; this is not a city!!!\n",
    "\n",
    "        if idx >= 1:\n",
    "            opt   = cities.click()\n",
    "            soup  = BeautifulSoup(driver.page_source)\n",
    "            links = soup.find_all('a', class_ = 'c-link')\n",
    "\n",
    "            for i in links:\n",
    "                if i.has_attr('href') and ('/api/' in i['href']):\n",
    "                    links_download.append('https://www.citylines.co' + i['href'])\n",
    "                    files_names.append(i['download'])\n",
    "        idx += 1\n",
    "    \n",
    "    driver.quit()\n",
    "\n",
    "    files_df = pd.DataFrame({'file_name':files_names, 'link':links_download})\n",
    "    \n",
    "    return files_df\n",
    "\n",
    "def download_data(files_df):\n",
    "    # download all the dataset\n",
    "    for index, row in files_df.iterrows():\n",
    "\n",
    "            file_name = localDataUrl + '/' + row['file_name']\n",
    "            print('Downloading... ' + str(index+1) +'/' + str(files_df.shape[0]) + ' : ' + file_name)\n",
    "\n",
    "            urllib.request.urlretrieve(row['link'], file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORK in progress....\n",
    "\"\"\"\"\n",
    "from pandas.io.json import json_normalize \n",
    "import json\n",
    "\n",
    "def mergeFiles(file_name, files_folder_path, main_node):\n",
    "    \n",
    "    \n",
    "    all_files = os.listdir(files_folder_path)\n",
    "    selected_files = [i for i in all_files if file_name in i]\n",
    "    \n",
    "    df_merged = pd.DataFrame()\n",
    "    \n",
    "    for fl in selected_files:\n",
    "        print('Merging file... ' + fl)\n",
    "        with open(files_folder_path + '/' + fl) as data_file:    \n",
    "            data = json.load(data_file)\n",
    "        df = pd.DataFrame()\n",
    "    \n",
    "        for i in data[main_node]:\n",
    "            df = pd.concat([df, json_normalize(i)], sort = True)\n",
    "\n",
    "        df = df.reset_index(drop = True)\n",
    "\n",
    "        # further expand all left dict in columns\n",
    "        cols_with_dict = []\n",
    "        for j in df.columns:\n",
    "            hasDict = \"<class 'dict'>\" in df[j].apply(lambda x: str(type(x[0])) if hasattr(x, '__len__') and len(x)> 0 else x).tolist()\n",
    "            cols_with_dict.append(hasDict)\n",
    "\n",
    "        for a, b in zip(cols_with_dict, df.columns):\n",
    "            if a:\n",
    "                dicts = [x[0] if len(x)>0 else {} for x in df[b]]\n",
    "                df = pd.concat([df, json_normalize(records)], axis = 1, sort = True).drop(b, axis = 1)\n",
    "\n",
    "        df['city'] = fl.split('_')[0]\n",
    "        df.columns = [s.split('.')[-1] for s in df.columns.values]\n",
    "        \n",
    "        \n",
    "        dfs = [df, df_merged]\n",
    "        if df_merged.shape[0] > 0:\n",
    "            df_merged = pd.concat(dfs, ignore_index=True)\n",
    "        else:\n",
    "            df_merged = df\n",
    "        #df_merged = pd.concat([df, df_merged], sort = True, ignore_index = True)\n",
    "        \n",
    "    \n",
    "    return df_merged\n",
    "\n",
    "\n",
    "\n",
    "df_stations = mergeFiles('stations.geojson', files_folder_path = localDataUrl, main_node = 'features')\n",
    "df_stations\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nested json files handling\n",
    "Here, the Amsterdam nested json stations is converted to a pandas DataFrame. The goal will be to automatize this and do it for all the downloaded data sets. The above code is intended to do it but is still to debug. At the end the goal is to have an unique .csv file comprising all the stations for all the cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TESTS\n",
    "\n",
    "with open('/Volumes/Disk2/Courses MA3/MA3 - ADA/AIRBNB data/Transports/amsterdam_stations.geojson') as data_file:    \n",
    "            data = json.load(data_file)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for i in data['features']:\n",
    "    df = pd.concat([df, json_normalize(i)], sort = True)\n",
    "    \n",
    "df = df.reset_index(drop = True)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "cols_with_dict = []\n",
    "for i in df.columns:\n",
    "    hasDict = \"<class 'dict'>\" in df[i].apply(lambda x: str(type(x[0])) if hasattr(x, '__len__') and len(x)> 0 else x).tolist()\n",
    "    cols_with_dict.append(hasDict)\n",
    "\n",
    "for a, b in zip(cols_with_dict, df.columns):\n",
    "    if a:\n",
    "        dicts = [x[0] if len(x)>0 else {} for x in df[b]]\n",
    "        df = pd.concat([df, json_normalize(records)], axis = 1, sort = True).drop(b, axis = 1)\n",
    "\n",
    "df.columns = [s.split('.')[-1] for s in df.columns.values]       \n",
    "\n",
    "df.to_csv('Amsterdam_stations.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
