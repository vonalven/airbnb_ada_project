{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import time\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.core.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "### MODIFY THIS, to store the DataSet: ###\n",
    "##########################################\n",
    "\n",
    "localDataUrl = '/Volumes/Disk2/Courses MA3/MA3 - ADA/AIRBNB data/DataSets'\n",
    "\n",
    "##########################################\n",
    "##########################################\n",
    "\n",
    "url = 'http://insideairbnb.com/get-the-data.html'\n",
    "\n",
    "response = get(url);\n",
    "soup = BeautifulSoup(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define scraping functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beautiful tutorial: https://srome.github.io/Parsing-HTML-Tables-in-Python-with-BeautifulSoup-and-pandas/\n",
    "\n",
    "def url_to_df(soup):\n",
    "    for d in soup.find_all(class_='archived'): \n",
    "        d.decompose()\n",
    "\n",
    "    url_content = pd.DataFrame()\n",
    "    col_names = []\n",
    "\n",
    "    for table in soup.find_all('table'):\n",
    "\n",
    "        # find table elements \n",
    "        for row in table.find_all('tr'):\n",
    "\n",
    "            # find columns names:\n",
    "            th_tags = row.find_all('th') \n",
    "\n",
    "            # if not empty\n",
    "            if len(th_tags) > 0:\n",
    "                if len(col_names) == 0:\n",
    "                    for t in th_tags:\n",
    "                        col_names.append(t.text)\n",
    "\n",
    "            # find rows contents\n",
    "            td_tags = row.find_all('td')\n",
    "\n",
    "            content = []\n",
    "            for column in td_tags:\n",
    "                try:\n",
    "                    for el in column:\n",
    "                        content.append(el.get('href'))\n",
    "                    content.append(column.text)\n",
    "                except Exception:\n",
    "                    content.append(column.text)\n",
    "                    # pass\n",
    "\n",
    "            # check if list is not empty\n",
    "            if content:\n",
    "                url_content = url_content.append(pd.Series(content), ignore_index = True)\n",
    "\n",
    "    col_names = ['Date Compiled', 'Country/City', 'URL', 'File Name', 'Description']\n",
    "    url_content.columns = col_names\n",
    "\n",
    "    # format date\n",
    "    url_content['Date Compiled'] = pd.to_datetime(url_content['Date Compiled'], errors='coerce')\n",
    "    url_content['Date Compiled'] = url_content['Date Compiled'].astype('str')\n",
    "\n",
    "    display(url_content.head())\n",
    "    \n",
    "    return url_content\n",
    "\n",
    "\n",
    "def download_website(url_content):\n",
    "    for index, row in url_content.iterrows():\n",
    "\n",
    "        file_name = localDataUrl + '/' + row['Date Compiled'] + '_' + row['Country/City'] + '_' + row['File Name']\n",
    "        print('Downloading... ' + str(index+1) +'/' + str(url_content.shape[0]) + ' : ' + file_name)\n",
    "\n",
    "        try:\n",
    "            [fileName, header] = urllib.request.urlretrieve(row['URL'], file_name)\n",
    "\n",
    "            # decompress gzip file\n",
    "            if fileName[-2:]=='gz':\n",
    "                input = gzip.GzipFile(fileName, 'rb')\n",
    "                s = input.read()\n",
    "                input.close()\n",
    "                # delete gzip file\n",
    "                os.remove(fileName)\n",
    "\n",
    "                savingName = fileName[0:-3]\n",
    "                if 'listings' in fileName:\n",
    "                    savingName = savingName[0:-4] + '_detailed.csv'\n",
    "                output = open(savingName, 'wb')\n",
    "                output.write(s)\n",
    "                output.close()\n",
    "\n",
    "        except Exception:\n",
    "            print('Download failed')\n",
    "    \n",
    "        # pause code for a second so that we are not spamming the website with requests. \n",
    "        # This helps to avoid getting flagged as a spammer\n",
    "        time.sleep(1)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape and download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_content = url_to_soup(soup)\n",
    "download_website(url_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# remove all the archived classes\n",
    "for d in soup.find_all(class_='archived'): \n",
    "    d.decompose()\n",
    "file_classes = soup.find_all(class_ = lambda value: value and value.startswith('table table'))\n",
    "\n",
    "for city in file_classes:\n",
    "    files_links = city.find_all('a', class_=None)\n",
    "    for link in files_links:\n",
    "        file_url = link.get('href')\n",
    "        # print(file_url)\n",
    "        \n",
    "r0 = file_classes[0].find_all('td', class_='')\n",
    "r0\n",
    "date = r0[0].text\n",
    "city = r0[1].text\n",
    "link = r0[2]\n",
    "file_name = r0[3].text\n",
    "test = r0[4].text\n",
    "t0 = link.find_all(href = True)\n",
    "for i in link:\n",
    "    print(i.get('href'))\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
