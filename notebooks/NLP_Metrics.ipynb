{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Language processing\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# Language processing with TextBlob\n",
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and preprocess sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = spark.read.load('../sample_parquet/first_1000/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare stopwords, stemmer and lemmatizer for messages preprocessing.\n",
    "en_stopwords = stopwords.words('english')\n",
    "en_stemmer = SnowballStemmer('english')\n",
    "en_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_messages = messages.filter(\"body != '[removed]' and body != '[deleted]'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_body(body, n_grams=1, left_pad_symbol=None, right_pad_symbol=None, lemmatizer=None, stemmer=None, \\\n",
    "                   stop_words=None, lemmatize_stop_words=False, stem_stop_words=False, remove_stop_words=False):\n",
    "    \"\"\"\n",
    "    Process the message bodies of the given rdd\n",
    "        \n",
    "    Parameters:\n",
    "        body: \n",
    "            string message body\n",
    "        n_gram: \n",
    "            size of the n_grams in the rdd output\n",
    "        lemmatizer: \n",
    "            lemmatizer to use on the message words. If None, words are not lemmatize\n",
    "        stemmer: \n",
    "            stemmer to use on the message words. If None, words are not stemmed.\n",
    "        stop_words: \n",
    "            list of words to consider as stop words\n",
    "        lemmatize_stop_words: \n",
    "            boolean to lemmatize stop words\n",
    "        stem_stop_words: \n",
    "            boolean to stem stop words\n",
    "        remove_stop_words: \n",
    "            boolean to remove stop words from the tokens\n",
    "        \n",
    "    Returns:\n",
    "        rdd of the form (parent_id, id, processed_msg_body)\n",
    "    \"\"\"\n",
    "    \n",
    "    if n_grams < 1:\n",
    "        raise ValueError(\"n_grams should be bigger than 1\")\n",
    "    \n",
    "    tknzr = TweetTokenizer()\n",
    "    tokens = tknzr.tokenize(body)\n",
    "    \n",
    "    if stop_words is None:\n",
    "        stop_words = []\n",
    "    if lemmatizer is not None and stemmer is not None:\n",
    "        if remove_stop_words:\n",
    "            tokens = [lemmatizer.lemmatize(stemmer.stem(token)) for token in tokens if token not in stop_words]\n",
    "        elif not lemmatize_stop_words and not stem_stop_words:\n",
    "            tokens = [lemmatizer.lemmatize(stemmer.stem(token)) if token not in stop_words else token for token in tokens]\n",
    "        elif not lemmatize_stop_words:\n",
    "            tokens = [lemmatizer.lemmatize(stemmer.stem(token)) if token not in stop_words else stemmer.stem(token) for token in tokens]\n",
    "        elif not stem_stop_words:\n",
    "            tokens = [lemmatizer.lemmatize(stemmer.stem(token)) if token not in stop_words else lemmatizer.lemmatize(token) for token in tokens]\n",
    "    elif lemmatizer is not None:\n",
    "        if remove_stop_words:\n",
    "            tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "        elif not lemmatize_stop_words:\n",
    "            tokens = [lemmatizer.lemmatize(token) if token not in stop_words else token for token in tokens]\n",
    "    elif stemmer is not None:\n",
    "        if remove_stop_words:\n",
    "            tokens = [stemmer.stem(token) for token in tokens if token not in stop_words]\n",
    "        elif not stem_stop_words is not None:\n",
    "            tokens = [stemmer.stem(token) if token not in stop_words else token for token in tokens]\n",
    "    elif stemmer is not None and lemmatizer is not None:\n",
    "            tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    if left_pad_symbol is not None and right_pad_symbol is not None:\n",
    "        tokens = list(nltk.ngrams(tokens, n_grams, True, True, left_pad_symbol, right_pad_symbol))\n",
    "    elif left_pad_symbol is not None:\n",
    "        tokens = list(nltk.ngrams(tokens, n_grams, pad_left=True, left_pad_symbol=left_pad_symbol))\n",
    "    elif right_pad_symbol is not None:\n",
    "        tokens = list(nltk.ngrams(tokens, n_grams, pad_right=True, right_pad_symbol=right_pad_symbol))\n",
    "    else:\n",
    "        tokens = list(nltk.ngrams(tokens, n_grams))\n",
    "\n",
    "    return [list(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.process_body(body, n_grams=1, left_pad_symbol=None, right_pad_symbol=None, lemmatizer=None, stemmer=None, stop_words=None, lemmatize_stop_words=False, stem_stop_words=False, remove_stop_words=False)>"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_body_udf = func.udf(process_body, ArrayType(ArrayType(StringType(), False), False))\n",
    "spark.udf.register('process_body', process_body, ArrayType(ArrayType(StringType(), False), False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence polarity using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.compute_nltk_polarity(msg_body)>"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_nltk_polarity(msg_body):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    msg_body = sid.polarity_scores(msg_body)\n",
    "    return msg_body\n",
    "\n",
    "compute_nltk_polarity_udf = func.udf(compute_nltk_polarity, MapType(StringType(), FloatType(), False))\n",
    "spark.udf.register('compute_nltk_polarity', compute_nltk_polarity_udf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_bodies = cleaned_messages.selectExpr('id', 'created_utc', \"compute_nltk_polarity(body) as scores\")\n",
    "sent_nltk_scores = sent_bodies.select('id', 'created_utc', 'scores.neg', 'scores.neu', 'scores.pos')\n",
    "sent_nltk_scores = sent_nltk_scores.toDF('id', 'created_utc', 'nltk_negativity', 'nltk_neutrality', 'nltk_positivity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---------------+---------------+---------------+\n",
      "|     id|created_utc|nltk_negativity|nltk_neutrality|nltk_positivity|\n",
      "+-------+-----------+---------------+---------------+---------------+\n",
      "|c595rma| 1341368093|          0.065|          0.935|            0.0|\n",
      "|c595rqe| 1341368108|            0.0|            1.0|            0.0|\n",
      "|c595rwc| 1341368128|            0.0|            1.0|            0.0|\n",
      "|c595sdr| 1341368193|            0.0|          0.878|          0.122|\n",
      "|c595sop| 1341368236|            0.0|            1.0|            0.0|\n",
      "|c595sui| 1341368256|            0.0|           0.84|           0.16|\n",
      "|c595t5u| 1341368301|           0.36|          0.443|          0.197|\n",
      "|c595ti7| 1341368347|            0.0|            1.0|            0.0|\n",
      "|c595u4r| 1341368440|            0.0|            1.0|            0.0|\n",
      "|c595ule| 1341368505|          0.377|          0.623|            0.0|\n",
      "|c595up4| 1341368519|            0.0|          0.408|          0.592|\n",
      "|c595v6i| 1341368588|          0.456|          0.544|            0.0|\n",
      "|c595w8b| 1341368743|            0.0|            1.0|            0.0|\n",
      "|c595whq| 1341368782|            0.0|            1.0|            0.0|\n",
      "|c595xbt| 1341368907|          0.121|          0.786|          0.093|\n",
      "|c595yos| 1341369104|          0.147|           0.63|          0.223|\n",
      "|c595ypd| 1341369107|            0.0|          0.385|          0.615|\n",
      "|c595z4z| 1341369174|            0.0|            1.0|            0.0|\n",
      "|c595zjd| 1341369231|          0.111|          0.801|          0.088|\n",
      "|c595zrv| 1341369266|            0.0|          0.408|          0.592|\n",
      "+-------+-----------+---------------+---------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sent_nltk_scores.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence polarity using TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using simple sentence polarity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.compute_blob_polarity(msg_body)>"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_blob_polarity(msg_body):\n",
    "    sentiment = TextBlob(msg_body).sentiment\n",
    "    return {'polarity': sentiment.polarity, 'subjectivity': sentiment.subjectivity}\n",
    "\n",
    "compute_blob_polarity_udf = func.udf(compute_blob_polarity, MapType(StringType(), FloatType(), False))\n",
    "spark.udf.register('compute_blob_polarity', compute_blob_polarity_udf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_blob_bodies = cleaned_messages.selectExpr('id', 'created_utc', \"compute_blob_polarity(body) as scores\")\n",
    "sent_blob_scores = sent_blob_bodies.select('id', 'created_utc', 'scores.polarity', 'scores.subjectivity')\n",
    "sent_blob_scores = sent_blob_scores.toDF('id', 'created_utc', 'text_blob_polarity', 'text_blob_subjectivity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------------------+----------------------+\n",
      "|     id|created_utc|text_blob_polarity|text_blob_subjectivity|\n",
      "+-------+-----------+------------------+----------------------+\n",
      "|c595rma| 1341368093|      -0.041666668|                 0.425|\n",
      "|c595rqe| 1341368108|               0.0|                   0.0|\n",
      "|c595rwc| 1341368128|        0.20454545|             0.6818182|\n",
      "|c595sdr| 1341368193|               0.5|                   0.5|\n",
      "|c595sop| 1341368236|               0.0|                   0.0|\n",
      "|c595sui| 1341368256|        0.06666667|                   0.3|\n",
      "|c595t5u| 1341368301|               1.0|                   1.0|\n",
      "|c595ti7| 1341368347|               0.0|                   0.0|\n",
      "|c595u4r| 1341368440|               0.0|                   0.5|\n",
      "|c595ule| 1341368505|       -0.15982144|            0.60892856|\n",
      "|c595up4| 1341368519|        0.43333334|             0.8333333|\n",
      "|c595v6i| 1341368588|       -0.41666666|             0.6666667|\n",
      "|c595w8b| 1341368743|               0.0|                   0.0|\n",
      "|c595whq| 1341368782|          -0.15625|               0.40625|\n",
      "|c595xbt| 1341368907|        -0.6041667|             0.7513889|\n",
      "|c595yos| 1341369104|               0.1|            0.23333333|\n",
      "|c595ypd| 1341369107|               0.4|                   0.5|\n",
      "|c595z4z| 1341369174|               0.0|                   0.0|\n",
      "|c595zjd| 1341369231|           0.03125|            0.46458334|\n",
      "|c595zrv| 1341369266|               0.7|                   0.6|\n",
      "+-------+-----------+------------------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sent_blob_scores.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using twitter trained positive/negative naive bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.compute_blob_class_polarity(msg_body)>"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_blob_class_polarity(msg_body):\n",
    "    pol_class = TextBlob(msg_body, analyzer=NaiveBayesAnalyzer()).sentiment\n",
    "    return {'classification': -1 if pol_class.classification == 'neg' else 1, 'p_pos': pol_class.p_pos, 'p_neg': pol_class.p_neg}\n",
    "\n",
    "compute_blob_class_polarity_udf = func.udf(compute_blob_class_polarity, MapType(StringType(), FloatType(), False))\n",
    "spark.udf.register('compute_blob_class_polarity', compute_blob_class_polarity_udf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_blob_class_bodies = cleaned_messages.selectExpr('id', 'created_utc', \"compute_blob_class_polarity(body) as scores\")\n",
    "sent_blob_class_scores = sent_blob_class_bodies.select('id', 'created_utc', 'scores.classification', 'scores.p_pos', 'scores.p_neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This does not finish, classifier takes too long\n",
    "# sent_blob_class_scores.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other metrics (Vulgarity, hate speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+--------------------+\n",
      "|     id|created_utc|              tokens|\n",
      "+-------+-----------+--------------------+\n",
      "|c595rma| 1341368093|[[The], [fear], [...|\n",
      "|c595rqe| 1341368108|     [[Upvote], [!]]|\n",
      "|c595rwc| 1341368128|[[It's], [real], ...|\n",
      "|c595sdr| 1341368193|[[What's], [he], ...|\n",
      "|c595sop| 1341368236|[[does], [it], [e...|\n",
      "|c595sui| 1341368256|[[Your], [user], ...|\n",
      "|c595t5u| 1341368301|[[nope], [:D], [b...|\n",
      "|c595ti7| 1341368347|[[Along], [with],...|\n",
      "|c595u4r| 1341368440|[[Those], [both],...|\n",
      "|c595ule| 1341368505|[[If], [you], [ar...|\n",
      "|c595up4| 1341368519|[[Because], [easy...|\n",
      "|c595v6i| 1341368588|[[Does], [no], [o...|\n",
      "|c595w8b| 1341368743|[[Has], [the], [t...|\n",
      "|c595whq| 1341368782|[[Just], [because...|\n",
      "|c595xbt| 1341368907|[[Also], [,], [do...|\n",
      "|c595yos| 1341369104|[[Meh], [,], [it'...|\n",
      "|c595ypd| 1341369107|[[You], [enjoy], ...|\n",
      "|c595z4z| 1341369174|[[[], [Finnish], ...|\n",
      "|c595zjd| 1341369231|[[>], [*], [It's]...|\n",
      "|c595zrv| 1341369266|[[Good], [insight...|\n",
      "+-------+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokens = cleaned_messages.selectExpr('id', 'created_utc', 'process_body(body) as tokens')\n",
    "tokens.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def count_matches(msg_grams, ref_grams, ref_grams_intensity=None):\n",
    "    msg_grams_joined = [' '.join(msg_gram) for msg_gram in msg_grams]\n",
    "    msg_grams_counter = Counter(msg_grams_joined)\n",
    "    count = 0.0\n",
    "    intensity = 0.0\n",
    "    for i, ref_gram in enumerate(ref_grams):\n",
    "        count = count + msg_grams_counter[ref_gram]\n",
    "        if ref_grams_intensity is not None:\n",
    "            intensity = intensity + msg_grams_counter[ref_gram] * ref_grams_intensity[i]\n",
    "    \n",
    "    if ref_grams_intensity is None:\n",
    "        return count\n",
    "    else: \n",
    "        return {'count':count, 'intensity':intensity}\"\"\"\n",
    "    \n",
    "def count_matches(msg_grams, ref_grams_counter, ref_grams_intensity=None):\n",
    "    msg_grams_joined = [' '.join(msg_gram) for msg_gram in msg_grams]\n",
    "    print(msg_grams_joined)\n",
    "    msg_grams_counter = Counter(msg_grams_joined)\n",
    "    res_counter = msg_grams_counter & ref_grams_counter\n",
    "    \n",
    "    if ref_grams_intensity is not None:\n",
    "        res_intensity = dict()\n",
    "        for w, occ in res_counter.items():\n",
    "            res_intensity[w] = occ * ref_grams_intensity[w]\n",
    "    \n",
    "    count = sum(res_counter.values())\n",
    "    if ref_grams_intensity is None:\n",
    "        return count\n",
    "    else:\n",
    "        intensity = sum(res_intensity.values())\n",
    "        return {'count':count, 'intensity':intensity}\n",
    "    \n",
    "def df_count_matches(gram_counter):\n",
    "    return func.udf(lambda c: count_matches(c, gram_counter), FloatType())\n",
    "\n",
    "def df_count_matches_intensity(gram_counter, intensity_dict):\n",
    "    return func.udf(lambda c: count_matches(c, gram_counter, intensity_dict), MapType(StringType(), FloatType()))\n",
    "\n",
    "#df_count_matches_udf = func.udf(df_count_matches, FloatType())\n",
    "#df_count_matches_intensity_udf = func.udf(df_count_matches_intensity, MapType(StringType(), FloatType()))\n",
    "\n",
    "#spark.udf.register('df_count_matches', df_count_matches_udf)\n",
    "#spark.udf.register('df_count_matches_intensity', df_count_matches_intensity_udf)\n",
    "\n",
    "def df_count_matches_sql(gram_counter, sql_fun_name):\n",
    "    udf = func.udf(lambda c: count_matches(c, gram_counter), FloatType())\n",
    "    spark.udf.register(sql_fun_name, udf)\n",
    "\n",
    "def df_count_matches_intensity_sql(gram_counter, intensity_dict, sql_fun_name):\n",
    "    udf = func.udf(lambda c: count_matches(c, gram_counter, intensity_dict), MapType(StringType(), FloatType()))\n",
    "    spark.udf.register(sql_fun_name, udf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vulgarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------+\n",
      "|      en_bad_words|gram_rank|\n",
      "+------------------+---------+\n",
      "|              2g1c|        1|\n",
      "|     2 girls 1 cup|        4|\n",
      "|    acrotomophilia|        1|\n",
      "|alabama hot pocket|        3|\n",
      "|  alaskan pipeline|        2|\n",
      "|              anal|        1|\n",
      "|         anilingus|        1|\n",
      "|              anus|        1|\n",
      "|           apeshit|        1|\n",
      "|          arsehole|        1|\n",
      "|               ass|        1|\n",
      "|           asshole|        1|\n",
      "|          assmunch|        1|\n",
      "|       auto erotic|        2|\n",
      "|        autoerotic|        1|\n",
      "|          babeland|        1|\n",
      "|       baby batter|        2|\n",
      "|        baby juice|        2|\n",
      "|          ball gag|        2|\n",
      "|        ball gravy|        2|\n",
      "+------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bad_words = spark.read.csv('../bad_words_lexicon/en.csv', header=True)\n",
    "bw_gram_rank = bad_words.withColumn('gram_rank', func.udf(lambda gram: len(gram.split()), IntegerType())(func.col('en_bad_words')))\n",
    "bw_gram_rank.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2g1c': inf,\n",
       " 'acrotomophilia': inf,\n",
       " 'anal': inf,\n",
       " 'anilingus': inf,\n",
       " 'anus': inf,\n",
       " 'apeshit': inf,\n",
       " 'arsehole': inf,\n",
       " 'ass': inf,\n",
       " 'asshole': inf,\n",
       " 'assmunch': inf,\n",
       " 'autoerotic': inf,\n",
       " 'babeland': inf,\n",
       " 'bangbros': inf,\n",
       " 'bareback': inf,\n",
       " 'barenaked': inf,\n",
       " 'bastard': inf,\n",
       " 'bastardo': inf,\n",
       " 'bastinado': inf,\n",
       " 'bbw': inf,\n",
       " 'bdsm': inf,\n",
       " 'beaner': inf,\n",
       " 'beaners': inf,\n",
       " 'bestiality': inf,\n",
       " 'bimbos': inf,\n",
       " 'birdlock': inf,\n",
       " 'bitch': inf,\n",
       " 'bitches': inf,\n",
       " 'blowjob': inf,\n",
       " 'blumpkin': inf,\n",
       " 'bollocks': inf,\n",
       " 'bondage': inf,\n",
       " 'boner': inf,\n",
       " 'boob': inf,\n",
       " 'boobs': inf,\n",
       " 'bukkake': inf,\n",
       " 'bulldyke': inf,\n",
       " 'bullshit': inf,\n",
       " 'bunghole': inf,\n",
       " 'busty': inf,\n",
       " 'butt': inf,\n",
       " 'buttcheeks': inf,\n",
       " 'butthole': inf,\n",
       " 'camgirl': inf,\n",
       " 'camslut': inf,\n",
       " 'camwhore': inf,\n",
       " 'carpetmuncher': inf,\n",
       " 'circlejerk': inf,\n",
       " 'clit': inf,\n",
       " 'clitoris': inf,\n",
       " 'clusterfuck': inf,\n",
       " 'cock': inf,\n",
       " 'cocks': inf,\n",
       " 'coprolagnia': inf,\n",
       " 'coprophilia': inf,\n",
       " 'cornhole': inf,\n",
       " 'coon': inf,\n",
       " 'coons': inf,\n",
       " 'creampie': inf,\n",
       " 'cum': inf,\n",
       " 'cumming': inf,\n",
       " 'cunnilingus': inf,\n",
       " 'cunt': inf,\n",
       " 'dafuq': inf,\n",
       " 'dank': inf,\n",
       " 'darkie': inf,\n",
       " 'daterape': inf,\n",
       " 'deepthroat': inf,\n",
       " 'dendrophilia': inf,\n",
       " 'dick': inf,\n",
       " 'dork': inf,\n",
       " 'dildo': inf,\n",
       " 'dingleberry': inf,\n",
       " 'dingleberries': inf,\n",
       " 'doggiestyle': inf,\n",
       " 'doggystyle': inf,\n",
       " 'dolcett': inf,\n",
       " 'domination': inf,\n",
       " 'dominatrix': inf,\n",
       " 'dommes': inf,\n",
       " 'douche': inf,\n",
       " 'douchebag': inf,\n",
       " 'dumbass': inf,\n",
       " 'dvda': inf,\n",
       " 'ecchi': inf,\n",
       " 'ejaculation': inf,\n",
       " 'erotic': inf,\n",
       " 'erotism': inf,\n",
       " 'escort': inf,\n",
       " 'eunuch': inf,\n",
       " 'fag': inf,\n",
       " 'faggot': inf,\n",
       " 'fecal': inf,\n",
       " 'felch': inf,\n",
       " 'fellatio': inf,\n",
       " 'feltch': inf,\n",
       " 'femdom': inf,\n",
       " 'figging': inf,\n",
       " 'fingerbang': inf,\n",
       " 'fingering': inf,\n",
       " 'fisting': inf,\n",
       " 'footjob': inf,\n",
       " 'frotting': inf,\n",
       " 'fuck': inf,\n",
       " 'fuckin': inf,\n",
       " 'fucking': inf,\n",
       " 'fucktards': inf,\n",
       " 'fudgepacker': inf,\n",
       " 'futanari': inf,\n",
       " 'genitals': inf,\n",
       " 'goatcx': inf,\n",
       " 'goatse': inf,\n",
       " 'gokkun': inf,\n",
       " 'goodpoop': inf,\n",
       " 'goregasm': inf,\n",
       " 'grope': inf,\n",
       " 'g-spot': inf,\n",
       " 'guro': inf,\n",
       " 'handjob': inf,\n",
       " 'hardcore': inf,\n",
       " 'hentai': inf,\n",
       " 'hoe': inf,\n",
       " 'homoerotic': inf,\n",
       " 'honkey': inf,\n",
       " 'hooker': inf,\n",
       " 'humping': inf,\n",
       " 'incest': inf,\n",
       " 'intercourse': inf,\n",
       " 'jailbait': inf,\n",
       " 'jigaboo': inf,\n",
       " 'jiggaboo': inf,\n",
       " 'jiggerboo': inf,\n",
       " 'jizz': inf,\n",
       " 'juggs': inf,\n",
       " 'kike': inf,\n",
       " 'kinbaku': inf,\n",
       " 'kinkster': inf,\n",
       " 'kinky': inf,\n",
       " 'knobbing': inf,\n",
       " 'lolita': inf,\n",
       " 'lovemaking': inf,\n",
       " 'masturbate': inf,\n",
       " 'milf': inf,\n",
       " 'motherfucker': inf,\n",
       " 'muffdiving': inf,\n",
       " 'nambla': inf,\n",
       " 'nawashi': inf,\n",
       " 'negro': inf,\n",
       " 'neonazi': inf,\n",
       " 'nigga': inf,\n",
       " 'nigger': inf,\n",
       " 'nimphomania': inf,\n",
       " 'nipple': inf,\n",
       " 'nipples': inf,\n",
       " 'nude': inf,\n",
       " 'nudity': inf,\n",
       " 'nympho': inf,\n",
       " 'nymphomania': inf,\n",
       " 'octopussy': inf,\n",
       " 'omorashi': inf,\n",
       " 'orgasm': inf,\n",
       " 'orgy': inf,\n",
       " 'paedophile': inf,\n",
       " 'paki': inf,\n",
       " 'panties': inf,\n",
       " 'panty': inf,\n",
       " 'pedobear': inf,\n",
       " 'pedophile': inf,\n",
       " 'pegging': inf,\n",
       " 'penis': inf,\n",
       " 'pissing': inf,\n",
       " 'pisspig': inf,\n",
       " 'playboy': inf,\n",
       " 'ponyplay': inf,\n",
       " 'poof': inf,\n",
       " 'poon': inf,\n",
       " 'poontang': inf,\n",
       " 'punany': inf,\n",
       " 'poopchute': inf,\n",
       " 'porn': inf,\n",
       " 'porno': inf,\n",
       " 'pornography': inf,\n",
       " 'pthc': inf,\n",
       " 'pubes': inf,\n",
       " 'pussy': inf,\n",
       " 'queaf': inf,\n",
       " 'queef': inf,\n",
       " 'quim': inf,\n",
       " 'raghead': inf,\n",
       " 'rape': inf,\n",
       " 'raping': inf,\n",
       " 'rapist': inf,\n",
       " 'rectum': inf,\n",
       " 'rimjob': inf,\n",
       " 'rimming': inf,\n",
       " 'sadism': inf,\n",
       " 'santorum': inf,\n",
       " 'scat': inf,\n",
       " 'schlong': inf,\n",
       " 'scissoring': inf,\n",
       " 'semen': inf,\n",
       " 'sex': inf,\n",
       " 'sexo': inf,\n",
       " 'sexy': inf,\n",
       " 'shemale': inf,\n",
       " 'shibari': inf,\n",
       " 'shit': inf,\n",
       " 'shitblimp': inf,\n",
       " 'shitty': inf,\n",
       " 'shota': inf,\n",
       " 'shrimping': inf,\n",
       " 'skeet': inf,\n",
       " 'slanteye': inf,\n",
       " 'slut': inf,\n",
       " 's&m': inf,\n",
       " 'smut': inf,\n",
       " 'snatch': inf,\n",
       " 'snowballing': inf,\n",
       " 'sodomize': inf,\n",
       " 'sodomy': inf,\n",
       " 'spic': inf,\n",
       " 'splooge': inf,\n",
       " 'spooge': inf,\n",
       " 'spunk': inf,\n",
       " 'strapon': inf,\n",
       " 'strappado': inf,\n",
       " 'suck': inf,\n",
       " 'sucks': inf,\n",
       " 'swastika': inf,\n",
       " 'swinger': inf,\n",
       " 'threesome': inf,\n",
       " 'throating': inf,\n",
       " 'tit': inf,\n",
       " 'tits': inf,\n",
       " 'titties': inf,\n",
       " 'titty': inf,\n",
       " 'topless': inf,\n",
       " 'tosser': inf,\n",
       " 'towelhead': inf,\n",
       " 'tranny': inf,\n",
       " 'tribadism': inf,\n",
       " 'tubgirl': inf,\n",
       " 'tushy': inf,\n",
       " 'twat': inf,\n",
       " 'twink': inf,\n",
       " 'twinkie': inf,\n",
       " 'undressing': inf,\n",
       " 'upskirt': inf,\n",
       " 'urophilia': inf,\n",
       " 'vagina': inf,\n",
       " 'vibrator': inf,\n",
       " 'vorarephilia': inf,\n",
       " 'voyeur': inf,\n",
       " 'vulva': inf,\n",
       " 'wank': inf,\n",
       " 'wetback': inf,\n",
       " 'whore': inf,\n",
       " 'xx': inf,\n",
       " 'xxx': inf,\n",
       " 'yaoi': inf,\n",
       " 'yiffy': inf,\n",
       " 'zoophilia': inf,\n",
       " '__': inf}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bw_1_grams = {i.en_bad_words:np.inf for i in bw_gram_rank.filter('gram_rank == 1').select('en_bad_words').collect()}\n",
    "bw_1_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-------------+\n",
      "|     id|created_utc|nb_bw_matches|\n",
      "+-------+-----------+-------------+\n",
      "|c595rma| 1341368093|          0.0|\n",
      "|c595rqe| 1341368108|          0.0|\n",
      "|c595rwc| 1341368128|          0.0|\n",
      "|c595sdr| 1341368193|          0.0|\n",
      "|c595sop| 1341368236|          0.0|\n",
      "|c595sui| 1341368256|          0.0|\n",
      "|c595t5u| 1341368301|          0.0|\n",
      "|c595ti7| 1341368347|          0.0|\n",
      "|c595u4r| 1341368440|          0.0|\n",
      "|c595ule| 1341368505|          0.0|\n",
      "|c595up4| 1341368519|          0.0|\n",
      "|c595v6i| 1341368588|          0.0|\n",
      "|c595w8b| 1341368743|          0.0|\n",
      "|c595whq| 1341368782|          0.0|\n",
      "|c595xbt| 1341368907|          0.0|\n",
      "|c595yos| 1341369104|          0.0|\n",
      "|c595ypd| 1341369107|          0.0|\n",
      "|c595z4z| 1341369174|          0.0|\n",
      "|c595zjd| 1341369231|          0.0|\n",
      "|c595zrv| 1341369266|          0.0|\n",
      "+-------+-----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bw_counter = tokens.withColumn(\"tokens\", df_count_matches(bw_1_grams)(func.col(\"tokens\"))).withColumnRenamed('tokens', 'nb_bw_matches')\n",
    "bw_counter.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hate speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw hate words (basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+\n",
      "|  hate_words|gram_rank|\n",
      "+------------+---------+\n",
      "|        gypo|        1|\n",
      "|       gypos|        1|\n",
      "|        cunt|        1|\n",
      "|       cunts|        1|\n",
      "|  peckerwood|        1|\n",
      "| peckerwoods|        1|\n",
      "|     raghead|        1|\n",
      "|    ragheads|        1|\n",
      "|     cripple|        1|\n",
      "|    cripples|        1|\n",
      "|      niggur|        1|\n",
      "|     niggurs|        1|\n",
      "| yellow bone|        2|\n",
      "|yellow bones|        2|\n",
      "|      muzzie|        1|\n",
      "|     muzzies|        1|\n",
      "|      niggar|        1|\n",
      "|     niggars|        1|\n",
      "|      nigger|        1|\n",
      "|     niggers|        1|\n",
      "+------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hate_words = spark.read.csv('../hatespeech_lexicon/hatebase_dict.csv', header=True)\n",
    "hate_words = hate_words.withColumnRenamed(\"uncivilised',\", 'hate_words') \\\n",
    "                        .withColumn('hate_words', func.udf(lambda d: d[1:-2])(func.col('hate_words')))\n",
    "hw_gram_rank = hate_words.withColumn('gram_rank', func.udf(lambda gram: len(gram.split()), IntegerType())(func.col('hate_words')))\n",
    "hw_gram_rank.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gypo': inf,\n",
       " 'gypos': inf,\n",
       " 'cunt': inf,\n",
       " 'cunts': inf,\n",
       " 'peckerwood': inf,\n",
       " 'peckerwoods': inf,\n",
       " 'raghead': inf,\n",
       " 'ragheads': inf,\n",
       " 'cripple': inf,\n",
       " 'cripples': inf,\n",
       " 'niggur': inf,\n",
       " 'niggurs': inf,\n",
       " 'muzzie': inf,\n",
       " 'muzzies': inf,\n",
       " 'niggar': inf,\n",
       " 'niggars': inf,\n",
       " 'nigger': inf,\n",
       " 'niggers': inf,\n",
       " 'greaseball': inf,\n",
       " 'greaseballs': inf,\n",
       " 'faggot': inf,\n",
       " 'faggots': inf,\n",
       " 'darkie': inf,\n",
       " 'darkies': inf,\n",
       " 'hoser': inf,\n",
       " 'hosers': inf,\n",
       " 'Jihadi': inf,\n",
       " 'Jihadis': inf,\n",
       " 'retard': inf,\n",
       " 'retards': inf,\n",
       " 'hillbilly': inf,\n",
       " 'hillbillies': inf,\n",
       " 'fag': inf,\n",
       " 'fags': inf,\n",
       " 'pikey': inf,\n",
       " 'pikies': inf,\n",
       " 'nicca': inf,\n",
       " 'niccas': inf,\n",
       " 'tranny': inf,\n",
       " 'trannies': inf,\n",
       " 'wigger': inf,\n",
       " 'wiggers': inf,\n",
       " 'wetback': inf,\n",
       " 'wetbacks': inf,\n",
       " 'nigglet': inf,\n",
       " 'nigglets': inf,\n",
       " 'wigga': inf,\n",
       " 'wiggas': inf,\n",
       " 'dhimmi': inf,\n",
       " 'dhimmis': inf,\n",
       " 'honkey': inf,\n",
       " 'honkies': inf,\n",
       " 'eurotrash': inf,\n",
       " 'eurotrashes': inf,\n",
       " 'yardie': inf,\n",
       " 'yardies': inf,\n",
       " 'niggah': inf,\n",
       " 'niggahes': inf,\n",
       " 'yokel': inf,\n",
       " 'yokels': inf,\n",
       " 'nigguh': inf,\n",
       " 'nigguhes': inf,\n",
       " 'honkie': inf,\n",
       " 'niglet': inf,\n",
       " 'niglets': inf,\n",
       " 'gyppo': inf,\n",
       " 'gyppos': inf,\n",
       " 'dyke': inf,\n",
       " 'dykes': inf,\n",
       " 'honky': inf,\n",
       " 'jiggaboo': inf,\n",
       " 'jiggaboos': inf,\n",
       " 'Chinaman': inf,\n",
       " 'Chinamans': inf,\n",
       " 'newfie': inf,\n",
       " 'newfies': inf,\n",
       " 'limey': inf,\n",
       " 'limies': inf,\n",
       " 'guala': inf,\n",
       " 'gualas': inf,\n",
       " 'whigger': inf,\n",
       " 'whiggers': inf,\n",
       " 'jigaboo': inf,\n",
       " 'jigaboos': inf,\n",
       " 'nig': inf,\n",
       " 'nigs': inf,\n",
       " 'Zionazi': inf,\n",
       " 'Zionazis': inf,\n",
       " 'niggress': inf,\n",
       " 'niggresses': inf,\n",
       " 'yobbo': inf,\n",
       " 'yobbos': inf,\n",
       " 'sperg': inf,\n",
       " 'spergs': inf,\n",
       " 'pommy': inf,\n",
       " 'pommies': inf,\n",
       " 'munter': inf,\n",
       " 'munters': inf,\n",
       " 'pommie': inf,\n",
       " 'gyp': inf,\n",
       " 'gyps': inf,\n",
       " 'twat': inf,\n",
       " 'twats': inf,\n",
       " 'queer': inf,\n",
       " 'queers': inf,\n",
       " 'darky': inf,\n",
       " 'khazar': inf,\n",
       " 'khazars': inf,\n",
       " 'gippo': inf,\n",
       " 'gippos': inf,\n",
       " 'skanger': inf,\n",
       " 'skangers': inf,\n",
       " 'beaner': inf,\n",
       " 'beaners': inf,\n",
       " 'quadroon': inf,\n",
       " 'quadroons': inf,\n",
       " 'Cushite': inf,\n",
       " 'Cushites': inf,\n",
       " 'cracker': inf,\n",
       " 'crackers': inf,\n",
       " 'pickaninny': inf,\n",
       " 'pickaninnies': inf,\n",
       " 'hick': inf,\n",
       " 'hicks': inf,\n",
       " 'redneck': inf,\n",
       " 'rednecks': inf,\n",
       " 'spiv': inf,\n",
       " 'spivs': inf,\n",
       " 'zipperhead': inf,\n",
       " 'zipperheads': inf,\n",
       " 'Kushite': inf,\n",
       " 'Kushites': inf,\n",
       " 'Shylock': inf,\n",
       " 'Shylocks': inf,\n",
       " 'gook': inf,\n",
       " 'gooks': inf,\n",
       " 'papist': inf,\n",
       " 'papists': inf,\n",
       " 'hymie': inf,\n",
       " 'hymies': inf,\n",
       " 'wog': inf,\n",
       " 'wogs': inf,\n",
       " 'scally': inf,\n",
       " 'scallies': inf,\n",
       " 'coon': inf,\n",
       " 'coons': inf,\n",
       " 'whitey': inf,\n",
       " 'whities': inf,\n",
       " 'nigette': inf,\n",
       " 'nigettes': inf,\n",
       " 'paki': inf,\n",
       " 'pakis': inf,\n",
       " 'Argie': inf,\n",
       " 'Argies': inf,\n",
       " 'wexican': inf,\n",
       " 'wexicans': inf,\n",
       " 'jigger': inf,\n",
       " 'jiggers': inf,\n",
       " 'injun': inf,\n",
       " 'injuns': inf,\n",
       " 'ocker': inf,\n",
       " 'ockers': inf,\n",
       " 'polack': inf,\n",
       " 'polacks': inf,\n",
       " 'moulie': inf,\n",
       " 'moulies': inf,\n",
       " 'niggor': inf,\n",
       " 'niggors': inf,\n",
       " 'scanger': inf,\n",
       " 'scangers': inf,\n",
       " 'ofay': inf,\n",
       " 'ofaies': inf,\n",
       " 'jigga': inf,\n",
       " 'jiggas': inf,\n",
       " 'redskin': inf,\n",
       " 'redskins': inf,\n",
       " 'chonky': inf,\n",
       " 'chonkies': inf,\n",
       " 'hebro': inf,\n",
       " 'hebros': inf,\n",
       " 'wop': inf,\n",
       " 'wops': inf,\n",
       " 'chink': inf,\n",
       " 'chinks': inf,\n",
       " 'paleface': inf,\n",
       " 'palefaces': inf,\n",
       " 'nigra': inf,\n",
       " 'nigras': inf,\n",
       " 'spic': inf,\n",
       " 'spics': inf,\n",
       " 'jocky': inf,\n",
       " 'jockies': inf,\n",
       " 'kraut': inf,\n",
       " 'krauts': inf,\n",
       " 'steek': inf,\n",
       " 'steeks': inf,\n",
       " 'coolie': inf,\n",
       " 'coolies': inf,\n",
       " 'gooky': inf,\n",
       " 'gookies': inf,\n",
       " 'octaroon': inf,\n",
       " 'octaroons': inf,\n",
       " 'bint': inf,\n",
       " 'bints': inf,\n",
       " 'squaw': inf,\n",
       " 'squaws': inf,\n",
       " 'Oriental': inf,\n",
       " 'Orientals': inf,\n",
       " 'halfrican': inf,\n",
       " 'halfricans': inf,\n",
       " 'paddy': inf,\n",
       " 'paddies': inf,\n",
       " 'groid': inf,\n",
       " 'groids': inf,\n",
       " 'jiggabo': inf,\n",
       " 'jiggabos': inf,\n",
       " 'jigg': inf,\n",
       " 'jiggs': inf,\n",
       " 'jant': inf,\n",
       " 'jants': inf,\n",
       " 'spide': inf,\n",
       " 'spides': inf,\n",
       " 'ZOG': inf,\n",
       " 'ZOGs': inf,\n",
       " 'heeb': inf,\n",
       " 'heebs': inf,\n",
       " 'piker': inf,\n",
       " 'pikers': inf,\n",
       " 'higger': inf,\n",
       " 'higgers': inf,\n",
       " 'lemonhead': inf,\n",
       " 'lemonheads': inf,\n",
       " 'Hun': inf,\n",
       " 'Huns': inf,\n",
       " 'popolo': inf,\n",
       " 'popolos': inf,\n",
       " 'jhant': inf,\n",
       " 'jhants': inf,\n",
       " 'eyetie': inf,\n",
       " 'eyeties': inf,\n",
       " 'mockey': inf,\n",
       " 'mockies': inf,\n",
       " 'Jap': inf,\n",
       " 'Japs': inf,\n",
       " 'redlegs': inf,\n",
       " 'mulignan': inf,\n",
       " 'mulignans': inf,\n",
       " 'jockie': inf,\n",
       " 'moulinyan': inf,\n",
       " 'moulinyans': inf,\n",
       " 'nigar': inf,\n",
       " 'nigars': inf,\n",
       " 'darkey': inf,\n",
       " 'gurrier': inf,\n",
       " 'gurriers': inf,\n",
       " 'lubra': inf,\n",
       " 'lubras': inf,\n",
       " 'Buckwheat': inf,\n",
       " 'Buckwheats': inf,\n",
       " 'mulato': inf,\n",
       " 'mulatos': inf,\n",
       " 'kyke': inf,\n",
       " 'kykes': inf,\n",
       " 'boonie': inf,\n",
       " 'boonies': inf,\n",
       " 'mick': inf,\n",
       " 'micks': inf,\n",
       " 'bluegum': inf,\n",
       " 'bluegums': inf,\n",
       " 'spigger': inf,\n",
       " 'spiggers': inf,\n",
       " 'kike': inf,\n",
       " 'kikes': inf,\n",
       " 'moulignon': inf,\n",
       " 'moulignons': inf,\n",
       " 'roundeye': inf,\n",
       " 'roundeyes': inf,\n",
       " 'ginzo': inf,\n",
       " 'ginzos': inf,\n",
       " 'Jewbacca': inf,\n",
       " 'Jewbaccas': inf,\n",
       " 'booner': inf,\n",
       " 'booners': inf,\n",
       " 'nigre': inf,\n",
       " 'nigres': inf,\n",
       " 'scallie': inf,\n",
       " 'niger': inf,\n",
       " 'nigers': inf,\n",
       " 'dinge': inf,\n",
       " 'dinges': inf,\n",
       " 'Leb': inf,\n",
       " 'Lebs': inf,\n",
       " 'Lebbo': inf,\n",
       " 'Lebbos': inf,\n",
       " 'sambo': inf,\n",
       " 'sambos': inf,\n",
       " 'Africoon': inf,\n",
       " 'Africoons': inf,\n",
       " 'gub': inf,\n",
       " 'gubs': inf,\n",
       " 'japie': inf,\n",
       " 'japies': inf,\n",
       " 'hairyback': inf,\n",
       " 'hairybacks': inf,\n",
       " 'lugan': inf,\n",
       " 'lugans': inf,\n",
       " 'blaxican': inf,\n",
       " 'blaxicans': inf,\n",
       " 'moke': inf,\n",
       " 'mokes': inf,\n",
       " 'nigor': inf,\n",
       " 'nigors': inf,\n",
       " 'Kushi': inf,\n",
       " 'Kushis': inf,\n",
       " 'hoosier': inf,\n",
       " 'hoosiers': inf,\n",
       " 'thicklips': inf,\n",
       " 'mook': inf,\n",
       " 'mooks': inf,\n",
       " 'muk': inf,\n",
       " 'muks': inf,\n",
       " 'senga': inf,\n",
       " 'sengas': inf,\n",
       " 'Cushi': inf,\n",
       " 'Cushis': inf,\n",
       " 'pogue': inf,\n",
       " 'pogues': inf,\n",
       " 'abo': inf,\n",
       " 'abos': inf,\n",
       " 'boong': inf,\n",
       " 'boongs': inf,\n",
       " 'dago': inf,\n",
       " 'dagos': inf,\n",
       " 'dogun': inf,\n",
       " 'doguns': inf,\n",
       " 'mocky': inf,\n",
       " 'poppadom': inf,\n",
       " 'poppadoms': inf,\n",
       " 'Gwat': inf,\n",
       " 'Gwats': inf,\n",
       " 'spook': inf,\n",
       " 'spooks': inf,\n",
       " 'Afro-Saxon': inf,\n",
       " 'Afro-Saxons': inf,\n",
       " 'guido': inf,\n",
       " 'guidos': inf,\n",
       " 'latrino': inf,\n",
       " 'latrinos': inf,\n",
       " 'lowlander': inf,\n",
       " 'lowlanders': inf,\n",
       " 'mockie': inf,\n",
       " 'moky': inf,\n",
       " 'mokies': inf,\n",
       " 'mosshead': inf,\n",
       " 'mossheads': inf,\n",
       " 'gyppy': inf,\n",
       " 'gyppies': inf,\n",
       " 'Americoon': inf,\n",
       " 'Americoons': inf,\n",
       " 'Hunyak': inf,\n",
       " 'Hunyaks': inf,\n",
       " 'slopehead': inf,\n",
       " 'slopeheads': inf,\n",
       " 'teabagger': inf,\n",
       " 'teabaggers': inf,\n",
       " 'Armo': inf,\n",
       " 'Armos': inf,\n",
       " 'bitch': inf,\n",
       " 'bitches': inf,\n",
       " 'greaser': inf,\n",
       " 'greasers': inf,\n",
       " 'Honyock': inf,\n",
       " 'Honyocks': inf,\n",
       " 'retarded': inf,\n",
       " 'semihole': inf,\n",
       " 'semiholes': inf,\n",
       " 'Amo': inf,\n",
       " 'Amos': inf,\n",
       " 'buckra': inf,\n",
       " 'buckras': inf,\n",
       " 'burrhead': inf,\n",
       " 'burrheads': inf,\n",
       " 'spigotty': inf,\n",
       " 'spigotties': inf,\n",
       " 'roofucker': inf,\n",
       " 'roofuckers': inf,\n",
       " 'wiggerette': inf,\n",
       " 'wiggerettes': inf,\n",
       " 'Buddhahead': inf,\n",
       " 'Buddhaheads': inf,\n",
       " 'Caublasian': inf,\n",
       " 'Caublasians': inf,\n",
       " 'golliwog': inf,\n",
       " 'golliwogs': inf,\n",
       " 'guinea': inf,\n",
       " 'guineas': inf,\n",
       " 'octroon': inf,\n",
       " 'octroons': inf,\n",
       " 'pohm': inf,\n",
       " 'pohms': inf,\n",
       " 'pussy': inf,\n",
       " 'pussies': inf,\n",
       " 'Russellite': inf,\n",
       " 'Russellites': inf,\n",
       " 'uncivilized': inf,\n",
       " 'Whipped': inf,\n",
       " 'albino': inf,\n",
       " 'albinos': inf,\n",
       " 'ape': inf,\n",
       " 'apes': inf,\n",
       " 'buckethead': inf,\n",
       " 'bucketheads': inf,\n",
       " 'chug': inf,\n",
       " 'chugs': inf,\n",
       " 'leprechaun': inf,\n",
       " 'leprechauns': inf,\n",
       " 'mutt': inf,\n",
       " 'mutts': inf,\n",
       " 'negro': inf,\n",
       " 'negros': inf,\n",
       " 'negroe': inf,\n",
       " 'nitchee': inf,\n",
       " 'nitchees': inf,\n",
       " 'sooty': inf,\n",
       " 'sooties': inf,\n",
       " 'spick': inf,\n",
       " 'spicks': inf,\n",
       " 'tinkard': inf,\n",
       " 'tinkards': inf,\n",
       " 'zigabo': inf,\n",
       " 'zigabos': inf,\n",
       " 'abbo': inf,\n",
       " 'abbos': inf,\n",
       " 'Anglo': inf,\n",
       " 'Anglos': inf,\n",
       " 'azn': inf,\n",
       " 'azns': inf,\n",
       " 'beaney': inf,\n",
       " 'beanies': inf,\n",
       " 'Bengali': inf,\n",
       " 'Bengalis': inf,\n",
       " 'bhrempti': inf,\n",
       " 'bhremptis': inf,\n",
       " 'bird': inf,\n",
       " 'birds': inf,\n",
       " 'blockhead': inf,\n",
       " 'blockheads': inf,\n",
       " 'boon': inf,\n",
       " 'boons': inf,\n",
       " 'boonga': inf,\n",
       " 'boongas': inf,\n",
       " 'boxhead': inf,\n",
       " 'boxheads': inf,\n",
       " 'brownie': inf,\n",
       " 'brownies': inf,\n",
       " 'buffie': inf,\n",
       " 'buffies': inf,\n",
       " 'bumblebee': inf,\n",
       " 'bumblebees': inf,\n",
       " 'bung': inf,\n",
       " 'bungs': inf,\n",
       " 'bunga': inf,\n",
       " 'bungas': inf,\n",
       " 'butterhead': inf,\n",
       " 'butterheads': inf,\n",
       " 'celestial': inf,\n",
       " 'celestials': inf,\n",
       " 'Charlie': inf,\n",
       " 'Charlies': inf,\n",
       " 'chigger': inf,\n",
       " 'chiggers': inf,\n",
       " 'chinig': inf,\n",
       " 'chinigs': inf,\n",
       " 'chunky': inf,\n",
       " 'chunkies': inf,\n",
       " 'clam': inf,\n",
       " 'clams': inf,\n",
       " 'clamhead': inf,\n",
       " 'clamheads': inf,\n",
       " 'colored': inf,\n",
       " 'coloured': inf,\n",
       " 'crow': inf,\n",
       " 'crows': inf,\n",
       " 'dego': inf,\n",
       " 'degos': inf,\n",
       " 'dink': inf,\n",
       " 'dinks': inf,\n",
       " 'dogan': inf,\n",
       " 'dogans': inf,\n",
       " 'domes': inf,\n",
       " 'eggplant': inf,\n",
       " 'eggplants': inf,\n",
       " 'Fairy': inf,\n",
       " 'Fairies': inf,\n",
       " 'fez': inf,\n",
       " 'fezs': inf,\n",
       " 'FOB': inf,\n",
       " 'FOBs': inf,\n",
       " 'fuzzy': inf,\n",
       " 'fuzzies': inf,\n",
       " 'gable': inf,\n",
       " 'gables': inf,\n",
       " 'Gerudo': inf,\n",
       " 'Gerudos': inf,\n",
       " 'gew': inf,\n",
       " 'gews': inf,\n",
       " 'ghetto': inf,\n",
       " 'ghettos': inf,\n",
       " 'gipp': inf,\n",
       " 'gipps': inf,\n",
       " 'gyppie': inf,\n",
       " 'heinie': inf,\n",
       " 'heinies': inf,\n",
       " 'ho': inf,\n",
       " 'hos': inf,\n",
       " 'hoe': inf,\n",
       " 'hoes': inf,\n",
       " 'Honyak': inf,\n",
       " 'Honyaks': inf,\n",
       " 'Hunkie': inf,\n",
       " 'Hunkies': inf,\n",
       " 'Hunky': inf,\n",
       " 'Hunyock': inf,\n",
       " 'Hunyocks': inf,\n",
       " 'ike': inf,\n",
       " 'ikes': inf,\n",
       " 'ikey': inf,\n",
       " 'ikies': inf,\n",
       " 'iky': inf,\n",
       " 'jig': inf,\n",
       " 'jigs': inf,\n",
       " 'jigarooni': inf,\n",
       " 'jigaroonis': inf,\n",
       " 'jijjiboo': inf,\n",
       " 'jijjiboos': inf,\n",
       " 'kotiya': inf,\n",
       " 'kotiyas': inf,\n",
       " 'mickey': inf,\n",
       " 'mickies': inf,\n",
       " 'moch': inf,\n",
       " 'moches': inf,\n",
       " 'mock': inf,\n",
       " 'mocks': inf,\n",
       " 'mong': inf,\n",
       " 'mongs': inf,\n",
       " 'monkey': inf,\n",
       " 'monkies': inf,\n",
       " 'Moor': inf,\n",
       " 'Moors': inf,\n",
       " 'moxy': inf,\n",
       " 'moxies': inf,\n",
       " 'muktuk': inf,\n",
       " 'muktuks': inf,\n",
       " 'mung': inf,\n",
       " 'mungs': inf,\n",
       " 'munt': inf,\n",
       " 'munts': inf,\n",
       " 'ned': inf,\n",
       " 'nichi': inf,\n",
       " 'nichis': inf,\n",
       " 'nichiwa': inf,\n",
       " 'nichiwas': inf,\n",
       " 'nidge': inf,\n",
       " 'nidges': inf,\n",
       " 'nip': inf,\n",
       " 'nips': inf,\n",
       " 'nitchie': inf,\n",
       " 'nitchies': inf,\n",
       " 'nitchy': inf,\n",
       " 'Orangie': inf,\n",
       " 'Orangies': inf,\n",
       " 'Oreo': inf,\n",
       " 'Oreos': inf,\n",
       " 'papoose': inf,\n",
       " 'papooses': inf,\n",
       " 'piky': inf,\n",
       " 'pinto': inf,\n",
       " 'pintos': inf,\n",
       " 'pollo': inf,\n",
       " 'pollos': inf,\n",
       " 'pom': inf,\n",
       " 'poms': inf,\n",
       " 'Punjab': inf,\n",
       " 'Punjabs': inf,\n",
       " 'rube': inf,\n",
       " 'rubes': inf,\n",
       " 'sawney': inf,\n",
       " 'sawnies': inf,\n",
       " 'scag': inf,\n",
       " 'scags': inf,\n",
       " 'seppo': inf,\n",
       " 'seppos': inf,\n",
       " 'septic': inf,\n",
       " 'septics': inf,\n",
       " 'shant': inf,\n",
       " 'shants': inf,\n",
       " 'sheeny': inf,\n",
       " 'sheenies': inf,\n",
       " 'sheepfucker': inf,\n",
       " 'sheepfuckers': inf,\n",
       " 'Shelta': inf,\n",
       " 'Sheltas': inf,\n",
       " 'shiner': inf,\n",
       " 'shiners': inf,\n",
       " 'Shy': inf,\n",
       " 'Shies': inf,\n",
       " 'skag': inf,\n",
       " 'skags': inf,\n",
       " 'Skippy': inf,\n",
       " 'Skippies': inf,\n",
       " 'slag': inf,\n",
       " 'slags': inf,\n",
       " 'slant': inf,\n",
       " 'slants': inf,\n",
       " 'slit': inf,\n",
       " 'slits': inf,\n",
       " 'slope': inf,\n",
       " 'slopes': inf,\n",
       " 'slopey': inf,\n",
       " 'slopies': inf,\n",
       " 'slopy': inf,\n",
       " 'sole': inf,\n",
       " 'soles': inf,\n",
       " 'spickaboo': inf,\n",
       " 'spickaboos': inf,\n",
       " 'spig': inf,\n",
       " 'spigs': inf,\n",
       " 'spik': inf,\n",
       " 'spiks': inf,\n",
       " 'spink': inf,\n",
       " 'spinks': inf,\n",
       " 'squarehead': inf,\n",
       " 'squareheads': inf,\n",
       " 'squinty': inf,\n",
       " 'squinties': inf,\n",
       " 'stovepipe': inf,\n",
       " 'stovepipes': inf,\n",
       " 'Taffy': inf,\n",
       " 'Taffies': inf,\n",
       " 'teapot': inf,\n",
       " 'teapots': inf,\n",
       " 'tenker': inf,\n",
       " 'tenkers': inf,\n",
       " 'tincker': inf,\n",
       " 'tinckers': inf,\n",
       " 'tinkar': inf,\n",
       " 'tinkars': inf,\n",
       " 'tinker': inf,\n",
       " 'tinkers': inf,\n",
       " 'tinkere': inf,\n",
       " 'tinkeres': inf,\n",
       " 'trash': inf,\n",
       " 'trashes': inf,\n",
       " 'Twinkie': inf,\n",
       " 'Twinkies': inf,\n",
       " 'tyncar': inf,\n",
       " 'tyncars': inf,\n",
       " 'tynekere': inf,\n",
       " 'tynekeres': inf,\n",
       " 'tynkard': inf,\n",
       " 'tynkards': inf,\n",
       " 'tynkare': inf,\n",
       " 'tynkares': inf,\n",
       " 'tynker': inf,\n",
       " 'tynkers': inf,\n",
       " 'tynkere': inf,\n",
       " 'tynkeres': inf,\n",
       " 'WASP': inf,\n",
       " 'WASPs': inf,\n",
       " 'Yank': inf,\n",
       " 'Yanks': inf,\n",
       " 'Yankee': inf,\n",
       " 'Yankees': inf,\n",
       " 'yellow': inf,\n",
       " 'yellows': inf,\n",
       " 'yid': inf,\n",
       " 'yids': inf,\n",
       " 'yob': inf,\n",
       " 'yobs': inf,\n",
       " 'zebra': inf,\n",
       " 'zebras': inf,\n",
       " 'zippohead': inf,\n",
       " 'zippoheads': inf,\n",
       " 'knacker': inf,\n",
       " 'knackers': inf,\n",
       " 'shyster': inf,\n",
       " 'shysters': inf,\n",
       " 'bogan': inf,\n",
       " 'bogans': inf,\n",
       " 'hayseed': inf,\n",
       " 'bludger': inf,\n",
       " 'bludgers': inf,\n",
       " 'charver': inf,\n",
       " 'charvers': inf,\n",
       " 'chav': inf,\n",
       " 'chavs': inf,\n",
       " 'sheister': inf,\n",
       " 'sheisters': inf,\n",
       " 'charva': inf,\n",
       " 'charvas': inf,\n",
       " 'gubba': inf,\n",
       " 'gubbas': inf,\n",
       " 'hebe': inf,\n",
       " 'hebes': inf,\n",
       " 'millie': inf,\n",
       " 'millies': inf,\n",
       " 'quashie': inf,\n",
       " 'quashies': inf,\n",
       " 'boojie': inf,\n",
       " 'boojies': inf,\n",
       " 'idiot': inf,\n",
       " 'idiots': inf,\n",
       " 'jock': inf,\n",
       " 'jocks': inf,\n",
       " 'mack': inf,\n",
       " 'macks': inf,\n",
       " 'Merkin': inf,\n",
       " 'Merkins': inf,\n",
       " 'neche': inf,\n",
       " 'neches': inf,\n",
       " 'neejee': inf,\n",
       " 'neejees': inf,\n",
       " 'neechee': inf,\n",
       " 'neechees': inf,\n",
       " 'powderburn': inf,\n",
       " 'powderburns': inf,\n",
       " 'proddywhoddy': inf,\n",
       " 'proddywhoddies': inf,\n",
       " 'proddywoddy': inf,\n",
       " 'proddywoddies': inf}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hw_1_grams = {i.hate_words: np.inf for i in hw_gram_rank.filter('gram_rank == 1').select('hate_words').collect()}\n",
    "hw_1_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-------------+\n",
      "|     id|created_utc|nb_hw_matches|\n",
      "+-------+-----------+-------------+\n",
      "|c595rma| 1341368093|          0.0|\n",
      "|c595rqe| 1341368108|          0.0|\n",
      "|c595rwc| 1341368128|          0.0|\n",
      "|c595sdr| 1341368193|          0.0|\n",
      "|c595sop| 1341368236|          0.0|\n",
      "|c595sui| 1341368256|          0.0|\n",
      "|c595t5u| 1341368301|          0.0|\n",
      "|c595ti7| 1341368347|          0.0|\n",
      "|c595u4r| 1341368440|          0.0|\n",
      "|c595ule| 1341368505|          1.0|\n",
      "|c595up4| 1341368519|          0.0|\n",
      "|c595v6i| 1341368588|          0.0|\n",
      "|c595w8b| 1341368743|          0.0|\n",
      "|c595whq| 1341368782|          0.0|\n",
      "|c595xbt| 1341368907|          0.0|\n",
      "|c595yos| 1341369104|          0.0|\n",
      "|c595ypd| 1341369107|          0.0|\n",
      "|c595z4z| 1341369174|          0.0|\n",
      "|c595zjd| 1341369231|          0.0|\n",
      "|c595zrv| 1341369266|          0.0|\n",
      "+-------+-----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hw_counter = tokens.withColumn(\"tokens\", df_count_matches(hw_1_grams)(func.col(\"tokens\"))).withColumnRenamed('tokens', 'nb_hw_matches')\n",
    "hw_counter.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refined hate words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+---------+\n",
      "|hate_words_ref|intensity|gram_rank|\n",
      "+--------------+---------+---------+\n",
      "|   allah akbar|     0.87|        2|\n",
      "|        blacks|    0.583|        1|\n",
      "|         chink|    0.467|        1|\n",
      "|        chinks|    0.542|        1|\n",
      "|         dykes|    0.602|        1|\n",
      "|        faggot|    0.489|        1|\n",
      "|       faggots|    0.675|        1|\n",
      "|          fags|    0.543|        1|\n",
      "|          homo|    0.667|        1|\n",
      "|        inbred|    0.583|        1|\n",
      "|        nigger|    0.584|        1|\n",
      "|       niggers|    0.672|        1|\n",
      "|        queers|      0.5|        1|\n",
      "|         raped|    0.717|        1|\n",
      "|       savages|    0.778|        1|\n",
      "|         slave|    0.667|        1|\n",
      "|          spic|     0.75|        1|\n",
      "|       wetback|    0.667|        1|\n",
      "|      wetbacks|    0.688|        1|\n",
      "|        whites|    0.556|        1|\n",
      "+--------------+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hw_ref_schema = StructType([StructField('hate_words_ref', StringType(), False), StructField('intensity', FloatType(), False)])\n",
    "hate_words_ref = spark.read.csv('../hatespeech_lexicon/refined_ngram_dict.csv', header=True, schema=hw_ref_schema)\n",
    "hw_ref_gram_rank = hate_words_ref.withColumn('gram_rank', func.udf(lambda gram: len(gram.split()), IntegerType())(func.col('hate_words_ref')))\n",
    "hw_ref_gram_rank.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'blacks': inf,\n",
       "  'chink': inf,\n",
       "  'chinks': inf,\n",
       "  'dykes': inf,\n",
       "  'faggot': inf,\n",
       "  'faggots': inf,\n",
       "  'fags': inf,\n",
       "  'homo': inf,\n",
       "  'inbred': inf,\n",
       "  'nigger': inf,\n",
       "  'niggers': inf,\n",
       "  'queers': inf,\n",
       "  'raped': inf,\n",
       "  'savages': inf,\n",
       "  'slave': inf,\n",
       "  'spic': inf,\n",
       "  'wetback': inf,\n",
       "  'wetbacks': inf,\n",
       "  'whites': inf},\n",
       " {'blacks': 0.5830000042915344,\n",
       "  'chink': 0.46700000762939453,\n",
       "  'chinks': 0.5419999957084656,\n",
       "  'dykes': 0.6019999980926514,\n",
       "  'faggot': 0.48899999260902405,\n",
       "  'faggots': 0.675000011920929,\n",
       "  'fags': 0.5429999828338623,\n",
       "  'homo': 0.6669999957084656,\n",
       "  'inbred': 0.5830000042915344,\n",
       "  'nigger': 0.5839999914169312,\n",
       "  'niggers': 0.671999990940094,\n",
       "  'queers': 0.5,\n",
       "  'raped': 0.7170000076293945,\n",
       "  'savages': 0.777999997138977,\n",
       "  'slave': 0.6669999957084656,\n",
       "  'spic': 0.75,\n",
       "  'wetback': 0.6669999957084656,\n",
       "  'wetbacks': 0.6880000233650208,\n",
       "  'whites': 0.5559999942779541})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hw_ref_1_grams = {i.hate_words_ref: np.inf for i in hw_ref_gram_rank.filter('gram_rank == 1').select('hate_words_ref').collect()}\n",
    "hw_ref_1_intensity = {i.hate_words_ref: i.intensity for i in hw_ref_gram_rank.filter('gram_rank == 1').select('hate_words_ref', 'intensity').collect()}\n",
    "hw_ref_1_grams, hw_ref_1_intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------------------+-----------------+\n",
      "|     id|created_utc|hate_ref_intensity|nb_hw_ref_matches|\n",
      "+-------+-----------+------------------+-----------------+\n",
      "|c595rma| 1341368093|               0.0|              0.0|\n",
      "|c595rqe| 1341368108|               0.0|              0.0|\n",
      "|c595rwc| 1341368128|               0.0|              0.0|\n",
      "|c595sdr| 1341368193|               0.0|              0.0|\n",
      "|c595sop| 1341368236|               0.0|              0.0|\n",
      "|c595sui| 1341368256|               0.0|              0.0|\n",
      "|c595t5u| 1341368301|               0.0|              0.0|\n",
      "|c595ti7| 1341368347|               0.0|              0.0|\n",
      "|c595u4r| 1341368440|               0.0|              0.0|\n",
      "|c595ule| 1341368505|               0.0|              0.0|\n",
      "|c595up4| 1341368519|               0.0|              0.0|\n",
      "|c595v6i| 1341368588|               0.0|              0.0|\n",
      "|c595w8b| 1341368743|               0.0|              0.0|\n",
      "|c595whq| 1341368782|               0.0|              0.0|\n",
      "|c595xbt| 1341368907|               0.0|              0.0|\n",
      "|c595yos| 1341369104|               0.0|              0.0|\n",
      "|c595ypd| 1341369107|               0.0|              0.0|\n",
      "|c595z4z| 1341369174|               0.0|              0.0|\n",
      "|c595zjd| 1341369231|               0.0|              0.0|\n",
      "|c595zrv| 1341369266|               0.0|              0.0|\n",
      "+-------+-----------+------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hw_ref_counter = tokens.withColumn(\"tokens\", df_count_matches_intensity(hw_ref_1_grams, hw_ref_1_intensity)(func.col(\"tokens\"))).withColumnRenamed('tokens', 'nb_hw_ref_matches')\n",
    "hw_ref_scores = hw_ref_counter.select('id', 'created_utc', 'nb_hw_ref_matches.intensity', 'nb_hw_ref_matches.count')\n",
    "hw_ref_scores = hw_ref_scores.toDF('id', 'created_utc', 'hate_ref_intensity', 'nb_hw_ref_matches')\n",
    "hw_ref_scores.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count_matches_sql(bw_1_grams, 'bw_count_matches')\n",
    "df_count_matches_sql(hw_1_grams, 'hw_count_matches')\n",
    "df_count_matches_intensity_sql(hw_ref_1_grams, hw_ref_1_intensity, 'hw_ref_count_matches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+--------------------+---------------+---------------+---------------+------------------+----------------------+-------------+-------------+----------------+-----------------+\n",
      "|     id|created_utc|                body|nltk_negativity|nltk_neutrality|nltk_positivity|text_blob_polarity|text_blob_subjectivity|nb_bw_matches|nb_hw_matches|hw_ref_intensity|nb_hw_ref_matches|\n",
      "+-------+-----------+--------------------+---------------+---------------+---------------+------------------+----------------------+-------------+-------------+----------------+-----------------+\n",
      "|c595rma| 1341368093|The fear that it'...|          0.065|          0.935|            0.0|      -0.041666668|                 0.425|          0.0|          0.0|             0.0|              0.0|\n",
      "|c595rqe| 1341368108|             Upvote!|            0.0|            1.0|            0.0|               0.0|                   0.0|          0.0|          0.0|             0.0|              0.0|\n",
      "|c595rwc| 1341368128|It's real, it's i...|            0.0|            1.0|            0.0|        0.20454545|             0.6818182|          0.0|          0.0|             0.0|              0.0|\n",
      "|c595sdr| 1341368193|What's he now... ...|            0.0|          0.878|          0.122|               0.5|                   0.5|          0.0|          0.0|             0.0|              0.0|\n",
      "|c595sop| 1341368236|  does it ever end?\n",
      "|            0.0|            1.0|            0.0|               0.0|                   0.0|          0.0|          0.0|             0.0|              0.0|\n",
      "|c595sui| 1341368256|Your user name ju...|            0.0|           0.84|           0.16|        0.06666667|                   0.3|          0.0|          0.0|             0.0|              0.0|\n",
      "|c595t5u| 1341368301|nope :D  but SC2S...|           0.36|          0.443|          0.197|               1.0|                   1.0|          0.0|          0.0|             0.0|              0.0|\n",
      "|c595ti7| 1341368347|Along with the re...|            0.0|            1.0|            0.0|               0.0|                   0.0|          0.0|          0.0|             0.0|              0.0|\n",
      "|c595u4r| 1341368440|Those both seem s...|            0.0|            1.0|            0.0|               0.0|                   0.5|          0.0|          0.0|             0.0|              0.0|\n",
      "|c595ule| 1341368505|If you are denyin...|          0.377|          0.623|            0.0|       -0.15982144|            0.60892856|          0.0|          1.0|             0.0|              0.0|\n",
      "|c595up4| 1341368519| Because easy karma.|            0.0|          0.408|          0.592|        0.43333334|             0.8333333|          0.0|          0.0|             0.0|              0.0|\n",
      "|c595v6i| 1341368588|Does no one serio...|          0.456|          0.544|            0.0|       -0.41666666|             0.6666667|          0.0|          0.0|             0.0|              0.0|\n",
      "|c595w8b| 1341368743|Has the timing be...|            0.0|            1.0|            0.0|               0.0|                   0.0|          0.0|          0.0|             0.0|              0.0|\n",
      "|c595whq| 1341368782|Just because spec...|            0.0|            1.0|            0.0|          -0.15625|               0.40625|          0.0|          0.0|             0.0|              0.0|\n",
      "|c595xbt| 1341368907|Also, don't feel ...|          0.121|          0.786|          0.093|        -0.6041667|             0.7513889|          0.0|          0.0|             0.0|              0.0|\n",
      "|c595yos| 1341369104|Meh, it's still n...|          0.147|           0.63|          0.223|               0.1|            0.23333333|          0.0|          0.0|             0.0|              0.0|\n",
      "|c595ypd| 1341369107|You enjoy face-si...|            0.0|          0.385|          0.615|               0.4|                   0.5|          0.0|          0.0|             0.0|              0.0|\n",
      "|c595z4z| 1341369174|[Finnish it. Now....|            0.0|            1.0|            0.0|               0.0|                   0.0|          0.0|          0.0|             0.0|              0.0|\n",
      "|c595zjd| 1341369231|&gt;*It's the ind...|          0.111|          0.801|          0.088|           0.03125|            0.46458334|          0.0|          0.0|             0.0|              0.0|\n",
      "|c595zrv| 1341369266|Good insight bro-...|            0.0|          0.408|          0.592|               0.7|                   0.6|          0.0|          0.0|             0.0|              0.0|\n",
      "+-------+-----------+--------------------+---------------+---------------+---------------+------------------+----------------------+-------------+-------------+----------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nlp_metrics_df = cleaned_messages.selectExpr('id', 'created_utc', 'body', 'process_body(body) as tokens')\n",
    "nlp_metrics_df = nlp_metrics_df.selectExpr('id', 'created_utc', 'body', \"compute_nltk_polarity(body) as nltk_scores\", \"compute_blob_polarity(body) as blob_scores\", \"bw_count_matches(tokens) as nb_bw_matches\", \"hw_count_matches(tokens) as nb_hw_matches\", \"hw_ref_count_matches(tokens) as hw_ref_matches\")\n",
    "nlp_metrics_df = nlp_metrics_df.selectExpr('id', 'created_utc', 'body', 'nltk_scores.neg as nltk_negativity', 'nltk_scores.neu as nltk_neutrality', 'nltk_scores.pos as nltk_positivity', 'blob_scores.polarity as text_blob_polarity', 'blob_scores.subjectivity as text_blob_subjectivity', 'nb_bw_matches', 'nb_hw_matches', 'hw_ref_matches.intensity as hw_ref_intensity', 'hw_ref_matches.count as nb_hw_ref_matches')\n",
    "nlp_metrics_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_metrics_df = nlp_metrics_df.withColumn('created_utc', func.from_unixtime(nlp_metrics_df['created_utc'], 'yyyy-MM-dd HH:mm:ss.SS').cast(DateType())) \\\n",
    "                               .withColumnRenamed('created_utc', 'creation_date')\n",
    "\n",
    "nlp_metrics_df.registerTempTable(\"nlp_metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_pandas = nlp_metrics_df.drop('body').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_nlp_metrics = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    creation_date,\n",
    "    \n",
    "    AVG(sum_nltk_negativity) OVER (\n",
    "        ORDER BY creation_date\n",
    "        RANGE BETWEEN 30 PRECEDING AND 30 FOLLOWING\n",
    "    ) AS nltk_negativity_60d_avg,\n",
    "    \n",
    "    AVG(sum_nltk_neutrality) OVER (\n",
    "        ORDER BY creation_date\n",
    "        RANGE BETWEEN 30 PRECEDING AND 30 FOLLOWING\n",
    "    ) AS nltk_neutrality_60d_avg,\n",
    "    \n",
    "    AVG(sum_nltk_positivity) OVER (\n",
    "        ORDER BY creation_date\n",
    "        RANGE BETWEEN 30 PRECEDING AND 30 FOLLOWING\n",
    "    ) AS nltk_positivity_60d_avg,\n",
    "    \n",
    "    AVG(sum_text_blob_polarity) OVER (\n",
    "        ORDER BY creation_date\n",
    "        RANGE BETWEEN 30 PRECEDING AND 30 FOLLOWING\n",
    "    ) AS text_blob_polarity_60d_avg,\n",
    "    \n",
    "    AVG(sum_text_blob_subjectivity) OVER (\n",
    "        ORDER BY creation_date\n",
    "        RANGE BETWEEN 30 PRECEDING AND 30 FOLLOWING\n",
    "    ) AS text_blob_subjectivity_60d_avg,\n",
    "    \n",
    "    AVG(sum_nb_bw_matches) OVER (\n",
    "        ORDER BY creation_date\n",
    "        RANGE BETWEEN 30 PRECEDING AND 30 FOLLOWING\n",
    "    ) AS nb_bw_matches_60d_avg,\n",
    "    \n",
    "    AVG(sum_nb_hw_matches) OVER (\n",
    "        ORDER BY creation_date\n",
    "        RANGE BETWEEN 30 PRECEDING AND 30 FOLLOWING\n",
    "    ) AS nb_hw_matches_60d_avg,\n",
    "    \n",
    "    AVG(sum_hw_ref_intensity) OVER (\n",
    "        ORDER BY creation_date\n",
    "        RANGE BETWEEN 30 PRECEDING AND 30 FOLLOWING\n",
    "    ) AS hw_ref_intensity_60d_avg,\n",
    "    \n",
    "    AVG(sum_nb_hw_ref_matches) OVER (\n",
    "        ORDER BY creation_date\n",
    "        RANGE BETWEEN 30 PRECEDING AND 30 FOLLOWING\n",
    "    ) AS nb_hw_ref_matches_60d_avg\n",
    "    \n",
    "FROM (\n",
    "    SELECT\n",
    "        creation_date,\n",
    "        SUM(nltk_negativity) AS sum_nltk_negativity,\n",
    "        SUM(nltk_neutrality) AS sum_nltk_neutrality,\n",
    "        SUM(nltk_positivity) AS sum_nltk_positivity,\n",
    "        SUM(text_blob_polarity) AS sum_text_blob_polarity,\n",
    "        SUM(text_blob_subjectivity) AS sum_text_blob_subjectivity, \n",
    "        SUM(nb_bw_matches) AS sum_nb_bw_matches,\n",
    "        SUM(nb_hw_matches) AS sum_nb_hw_matches,\n",
    "        SUM(hw_ref_intensity) AS sum_hw_ref_intensity,\n",
    "        SUM(nb_hw_ref_matches) AS sum_nb_hw_ref_matches\n",
    "    FROM nlp_metrics\n",
    "    GROUP BY creation_date\n",
    "    ORDER BY creation_date\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset starts on the 2005-12-12 and ends on the 2017-04-01.\n"
     ]
    }
   ],
   "source": [
    "date_extrema = spark.sql(\"\"\"\n",
    "SELECT MIN(creation_date), MAX(creation_date)\n",
    "FROM nlp_metrics\n",
    "\"\"\").collect()\n",
    "min_date = date_extrema[0][0]\n",
    "max_date = date_extrema[0][1]\n",
    "\n",
    "print(\"The dataset starts on the {} and ends on the {}.\".format(min_date, max_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 5757823 messages\n"
     ]
    }
   ],
   "source": [
    "total_messages = spark.sql(\"\"\"\n",
    "SELECT COUNT(*)\n",
    "FROM nlp_metrics\n",
    "\"\"\").collect()\n",
    "\n",
    "tot_msg = total_messages[0][0]\n",
    "\n",
    "print(\"The dataset contains {} messages\".format(tot_msg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over all the messages, there are at least 589284.0 bad words, 106483.0 hate speech words, 15617.0 refined hate speech words\n"
     ]
    }
   ],
   "source": [
    "total_no_respect = spark.sql(\"\"\"\n",
    "SELECT SUM(nb_bw_matches), SUM(nb_hw_matches), SUM(nb_hw_ref_matches)\n",
    "FROM nlp_metrics\n",
    "\"\"\").collect()\n",
    "\n",
    "sum_bw = total_no_respect[0][0]\n",
    "sum_hw = total_no_respect[0][1]\n",
    "sum_ref_hw = total_no_respect[0][2]\n",
    "\n",
    "print(\"Over all the messages, there are at least {} bad words, {} hate speech words, {} refined hate speech words\".format(sum_bw, sum_hw, sum_ref_hw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_polarity = spark.sql(\"\"\"\n",
    "SELECT SUM(nltk_negativity), SUM(nltk_neutrality), SUM(nltk_positivity), SUM(text_blob_polarity), SUM(text_blob_subjectivity)\n",
    "FROM nlp_metrics\n",
    "\"\"\").collect()\n",
    "\n",
    "sum_nltk_neg = total_polarity[0][0]\n",
    "sum_nltk_neu = total_polarity[0][1]\n",
    "sum_nltk_pos = total_polarity[0][2]\n",
    "sum_txt_blob_pol = total_polarity[0][3]\n",
    "sum_txt_blob_subj = total_polarity[0][4]\n",
    "\n",
    "print(\"The total pos/neu/neg score of the dataset is: negativity: {}, neutrality:{}, positivity: {}\".format(nltk_neg, nltk_neu, nltk_pos))\n",
    "print(\"The total polarity and subjectivity score of the dataset is: polarity: {}, subjectivity: {}\".format(sum_txt_blob_pol, sum_txt_blob_subj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
