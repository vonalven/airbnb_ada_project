{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import urllib.request\n",
    "import json\n",
    "import pandas as pd\n",
    "import itertools \n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFY THIS, to store the DataSet:\n",
    "localDataUrl = '/Volumes/Disk2/Courses MA3/MA3 - ADA/AIRBNB data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://insideairbnb.com/get-the-data.html'\n",
    "\n",
    "response = get(url);\n",
    "soup = BeautifulSoup(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beautiful tutorial: https://srome.github.io/Parsing-HTML-Tables-in-Python-with-BeautifulSoup-and-pandas/\n",
    "\n",
    "for d in soup.find_all(class_='archived'): \n",
    "    d.decompose()\n",
    "\n",
    "url_content = pd.DataFrame()\n",
    "col_names = []\n",
    "\n",
    "for table in soup.find_all('table'):\n",
    "    \n",
    "    # find table elements \n",
    "    for row in table.find_all('tr'):\n",
    "        \n",
    "        # find columns names:\n",
    "        th_tags = row.find_all('th') \n",
    "        \n",
    "        # if not empty\n",
    "        if len(th_tags) > 0:\n",
    "            if len(col_names) == 0:\n",
    "                for t in th_tags:\n",
    "                    col_names.append(t.text)\n",
    "                \n",
    "        # find rows contents\n",
    "        td_tags = row.find_all('td')\n",
    "        \n",
    "        content = []\n",
    "        for column in td_tags:\n",
    "            try:\n",
    "                for el in column:\n",
    "                    content.append(el.get('href'))\n",
    "                content.append(column.text)\n",
    "            except Exception:\n",
    "                content.append(column.text)\n",
    "                # pass\n",
    "                \n",
    "        # check if list is not empty\n",
    "        if content:\n",
    "            url_content = url_content.append(pd.Series(content), ignore_index = True)\n",
    "\n",
    "col_names = ['Date Compiled', 'Country/City', 'URL', 'File Name', 'Description']\n",
    "url_content.columns = col_names\n",
    "\n",
    "# format date\n",
    "url_content['Date Compiled'] = pd.to_datetime(url_content['Date Compiled'], errors='coerce')\n",
    "url_content['Date Compiled'] = url_content['Date Compiled'].astype('str')\n",
    "\n",
    "url_content.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for index, row in url_content.iterrows():\n",
    "    \n",
    "    file_name = localDataUrl + '/' + row['Date Compiled'] + '_' + row['Country/City'] + '_' + row['File Name']\n",
    "    urllib.request.urlretrieve(row['File Name'], file_name)\n",
    "    print('Downloading: ' + file_name)\n",
    "    \n",
    "    # pause code for a second so that we are not spamming the website with requests. \n",
    "    # This helps to avoid getting flagged as a spammer\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# remove all the archived classes\n",
    "for d in soup.find_all(class_='archived'): \n",
    "    d.decompose()\n",
    "file_classes = soup.find_all(class_ = lambda value: value and value.startswith('table table'))\n",
    "\n",
    "for city in file_classes:\n",
    "    files_links = city.find_all('a', class_=None)\n",
    "    for link in files_links:\n",
    "        file_url = link.get('href')\n",
    "        # print(file_url)\n",
    "        \n",
    "r0 = file_classes[0].find_all('td', class_='')\n",
    "r0\n",
    "date = r0[0].text\n",
    "city = r0[1].text\n",
    "link = r0[2]\n",
    "file_name = r0[3].text\n",
    "test = r0[4].text\n",
    "t0 = link.find_all(href = True)\n",
    "for i in link:\n",
    "    print(i.get('href'))\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
